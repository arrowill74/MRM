{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import math\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Helper function\n",
    "def newPath(path):\n",
    "    if not os.path.isdir(path):\n",
    "        os.mkdir(path)\n",
    "        \n",
    "def writeProgress(msg, count, total):\n",
    "    sys.stdout.write(msg + \"{:.2%}\\r\".format(count/total))\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return np.maximum(0,x)  \n",
    "\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x)\n",
    "    softmax_x = exp_x / np.sum(exp_x)\n",
    "    return softmax_x \n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All features: (165, 4876)\n",
      "Movie genre: (165, 20)\n",
      "User following: (1582, 165)\n",
      "User genre: (1582, 20)\n"
     ]
    }
   ],
   "source": [
    "all_npy = np.load('./npy/all_4876.npy')\n",
    "movie_genre = np.load('./npy/movie_genre.npy')\n",
    "usr_following = np.load('./npy/user_followings.npy')\n",
    "usr_genre = np.load('./npy/user_genre.npy')\n",
    "\n",
    "print('All features:', all_npy.shape)\n",
    "print('Movie genre:', movie_genre.shape)\n",
    "print('User following:', usr_following.shape)\n",
    "print('User genre:', usr_genre.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize usr_genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1582, 20)\n"
     ]
    }
   ],
   "source": [
    "usr_genre_norm = np.zeros(usr_genre.shape)\n",
    "for i in range(len(usr_genre)):\n",
    "    usr_genre_norm[i] = usr_genre[i]/np.max(usr_genre[i])\n",
    "print(usr_genre_norm.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: [[2 1 0 ... 1 0 0]\n",
      " [4 8 4 ... 0 0 0]\n",
      " [2 2 2 ... 1 0 0]\n",
      " ...\n",
      " [5 3 0 ... 1 1 0]\n",
      " [2 2 0 ... 0 1 0]\n",
      " [3 2 0 ... 1 1 0]]\n",
      "After: [[0.22222222 0.11111111 0.         ... 0.11111111 0.         0.        ]\n",
      " [0.44444444 0.88888889 0.44444444 ... 0.         0.         0.        ]\n",
      " [0.4        0.4        0.4        ... 0.2        0.         0.        ]\n",
      " ...\n",
      " [0.26315789 0.15789474 0.         ... 0.05263158 0.05263158 0.        ]\n",
      " [0.28571429 0.28571429 0.         ... 0.         0.14285714 0.        ]\n",
      " [0.33333333 0.22222222 0.         ... 0.11111111 0.11111111 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print('Before:', usr_genre)\n",
    "print('After:', usr_genre_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training & testing split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min number of followers: 1\n",
      "Max number of followers: 520\n",
      "[520 487 442 431 394 393 384 384 382 368 357 348 342 341 340 331 327 320\n",
      " 317 307 300 291 288 287 278 277 275 273 260 256 254 239 232 230 230 218\n",
      " 217 217 210 208 207 200 193 192 183 182 178 173 172 170 169 168 167 164\n",
      " 162 159 152 150 149 148 146 146 145 143 142 138 136 134 134 132 126 126\n",
      " 125 125 124 123 123 122 122 119 118 117 116 114 110 110 109 107 105  97\n",
      "  97  96  93  90  89  89  83  82  82  81  80  78  78  78  77  76  75  73\n",
      "  73  72  72  71  70  68  66  64  64  63  61  61  61  59  58  58  52  49\n",
      "  49  48  46  44  43  41  41  40  40  39  37  37  37  35  33  32  31  31\n",
      "  29  28  26  25  24  23  23  22  20  20  19  19  18  18  14  12  11  11\n",
      "   9   4   1]\n",
      "The num of followers over 5: 163\n"
     ]
    }
   ],
   "source": [
    "#The number of followers for each movie\n",
    "moive_followers = np.sum(usr_following, axis=0)\n",
    "# print(moive_followers)\n",
    "\n",
    "print('Min number of followers:', np.min(moive_followers))\n",
    "print('Max number of followers:', np.max(moive_followers))\n",
    "\n",
    "asc = np.sort(moive_followers)\n",
    "# print(each_user)\n",
    "# print(asc)\n",
    "desc = np.flip(asc)\n",
    "print(desc)\n",
    "\n",
    "over5 = 0\n",
    "for num in moive_followers:\n",
    "    if num >= 5:\n",
    "        over5 += 1\n",
    "print('The num of followers over 5:', over5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min number of followings: 10\n",
      "Max number of followings: 133\n",
      "[133  88  86  61  59  55  52  52  48  47  45  45  44  44  44  43  43  43\n",
      "  43  42  42  42  42  42  42  41  41  41  41  41  41  40  40  39  39  39\n",
      "  38  37  36  36  35  35  35  34  34  34  34  34  33  33  32  32  32  32\n",
      "  32  31  31  31  31  31  31  31  31  31  31  30  30  30  30  30  30  30\n",
      "  30  29  29  29  29  29  29  28  27  27  27  27  27  27  27  26  26  26\n",
      "  26  26  26  26  26  26  26  26  26  26  26  26  26  26  26  26  25  25\n",
      "  25  25  25  25  25  25  25  25  25  25  25  25  25  25  24  24  24  24\n",
      "  24  24  24  24  24  24  24  24  24  24  24  23  23  23  23  23  23  23\n",
      "  23  23  23  23  23  23  23  23  23  23  23  23  22  22  22  22  22  22\n",
      "  22  22  22  22  22  22  22  22  22  22  22  22  22  22  22  22  21  21\n",
      "  21  21  21  21  21  21  21  21  21  21  21  21  21  21  21  21  21  21\n",
      "  21  20  20  20  20  20  20  20  20  20  20  20  20  20  20  20  20  20\n",
      "  20  20  20  20  20  20  20  20  20  20  20  20  20  19  19  19  19  19\n",
      "  19  19  19  19  19  19  19  19  19  19  19  19  19  19  19  19  19  19\n",
      "  19  19  19  19  19  19  19  19  19  19  19  18  18  18  18  18  18  18\n",
      "  18  18  18  18  18  18  18  18  18  18  18  18  18  18  18  18  18  18\n",
      "  18  18  18  18  18  18  18  18  18  18  18  18  18  18  18  18  18  18\n",
      "  18  18  18  18  18  18  18  18  18  17  17  17  17  17  17  17  17  17\n",
      "  17  17  17  17  17  17  17  17  17  17  17  17  17  17  17  17  17  17\n",
      "  17  17  17  17  17  17  17  17  17  17  17  17  17  17  17  17  17  17\n",
      "  17  17  17  17  17  17  17  17  17  17  17  17  17  17  16  16  16  16\n",
      "  16  16  16  16  16  16  16  16  16  16  16  16  16  16  16  16  16  16\n",
      "  16  16  16  16  16  16  16  16  16  16  16  16  16  16  16  16  16  16\n",
      "  16  16  16  16  16  16  16  16  16  16  16  16  16  16  16  16  16  16\n",
      "  16  16  16  16  16  16  16  16  15  15  15  15  15  15  15  15  15  15\n",
      "  15  15  15  15  15  15  15  15  15  15  15  15  15  15  15  15  15  15\n",
      "  15  15  15  15  15  15  15  15  15  15  15  15  15  15  15  15  15  15\n",
      "  15  15  15  15  15  15  15  15  15  15  15  15  15  15  15  15  15  15\n",
      "  15  15  15  15  15  15  15  15  14  14  14  14  14  14  14  14  14  14\n",
      "  14  14  14  14  14  14  14  14  14  14  14  14  14  14  14  14  14  14\n",
      "  14  14  14  14  14  14  14  14  14  14  14  14  14  14  14  14  14  14\n",
      "  14  14  14  14  14  14  14  14  14  14  14  14  14  14  14  14  14  14\n",
      "  14  14  14  14  14  14  14  14  14  14  14  14  14  14  14  14  14  14\n",
      "  14  14  14  14  14  14  14  14  14  14  14  14  14  14  14  14  14  14\n",
      "  14  13  13  13  13  13  13  13  13  13  13  13  13  13  13  13  13  13\n",
      "  13  13  13  13  13  13  13  13  13  13  13  13  13  13  13  13  13  13\n",
      "  13  13  13  13  13  13  13  13  13  13  13  13  13  13  13  13  13  13\n",
      "  13  13  13  13  13  13  13  13  13  13  13  13  13  13  13  13  13  13\n",
      "  13  13  13  13  13  13  13  13  13  13  13  13  13  13  13  13  13  13\n",
      "  13  13  13  13  13  13  13  13  13  13  13  13  13  13  13  13  13  13\n",
      "  13  13  13  13  13  13  13  13  13  13  13  13  13  13  13  13  13  13\n",
      "  13  13  13  13  13  13  13  13  13  13  13  13  13  13  13  13  13  13\n",
      "  13  13  13  13  12  12  12  12  12  12  12  12  12  12  12  12  12  12\n",
      "  12  12  12  12  12  12  12  12  12  12  12  12  12  12  12  12  12  12\n",
      "  12  12  12  12  12  12  12  12  12  12  12  12  12  12  12  12  12  12\n",
      "  12  12  12  12  12  12  12  12  12  12  12  12  12  12  12  12  12  12\n",
      "  12  12  12  12  12  12  12  12  12  12  12  12  12  12  12  12  12  12\n",
      "  12  12  12  12  12  12  12  12  12  12  12  12  12  12  12  12  12  12\n",
      "  12  12  12  12  12  12  12  12  12  12  12  12  12  12  12  12  12  12\n",
      "  12  12  12  12  12  12  12  12  12  12  12  12  12  12  12  12  12  12\n",
      "  12  12  12  12  12  12  12  12  12  12  12  12  12  12  12  12  12  12\n",
      "  12  12  12  12  12  12  12  12  12  12  12  12  12  12  12  12  12  12\n",
      "  12  11  11  11  11  11  11  11  11  11  11  11  11  11  11  11  11  11\n",
      "  11  11  11  11  11  11  11  11  11  11  11  11  11  11  11  11  11  11\n",
      "  11  11  11  11  11  11  11  11  11  11  11  11  11  11  11  11  11  11\n",
      "  11  11  11  11  11  11  11  11  11  11  11  11  11  11  11  11  11  11\n",
      "  11  11  11  11  11  11  11  11  11  11  11  11  11  11  11  11  11  11\n",
      "  11  11  11  11  11  11  11  11  11  11  11  11  11  11  11  11  11  11\n",
      "  11  11  11  11  11  11  11  11  11  11  11  11  11  11  11  11  11  11\n",
      "  11  11  11  11  11  11  11  11  11  11  11  11  11  11  11  11  11  11\n",
      "  11  11  11  11  11  11  11  11  11  11  11  11  11  11  11  11  11  11\n",
      "  11  11  11  11  11  11  11  11  11  11  11  11  11  11  11  11  11  11\n",
      "  11  11  11  11  11  11  11  11  11  11  11  11  11  11  11  11  11  11\n",
      "  11  11  11  11  11  11  11  11  11  11  11  11  11  11  11  11  11  11\n",
      "  11  11  11  11  11  11  11  11  11  11  11  11  11  11  11  11  11  11\n",
      "  11  11  11  11  11  11  11  11  11  11  11  11  11  11  11  11  11  11\n",
      "  10  10  10  10  10  10  10  10  10  10  10  10  10  10  10  10  10  10\n",
      "  10  10  10  10  10  10  10  10  10  10  10  10  10  10  10  10  10  10\n",
      "  10  10  10  10  10  10  10  10  10  10  10  10  10  10  10  10  10  10\n",
      "  10  10  10  10  10  10  10  10  10  10  10  10  10  10  10  10  10  10\n",
      "  10  10  10  10  10  10  10  10  10  10  10  10  10  10  10  10  10  10\n",
      "  10  10  10  10  10  10  10  10  10  10  10  10  10  10  10  10  10  10\n",
      "  10  10  10  10  10  10  10  10  10  10  10  10  10  10  10  10  10  10\n",
      "  10  10  10  10  10  10  10  10  10  10  10  10  10  10  10  10  10  10\n",
      "  10  10  10  10  10  10  10  10  10  10  10  10  10  10  10  10  10  10\n",
      "  10  10  10  10  10  10  10  10  10  10  10  10  10  10  10  10  10  10\n",
      "  10  10  10  10  10  10  10  10  10  10  10  10  10  10  10  10  10  10\n",
      "  10  10  10  10  10  10  10  10  10  10  10  10  10  10  10  10  10  10\n",
      "  10  10  10  10  10  10  10  10  10  10  10  10  10  10  10  10  10  10\n",
      "  10  10  10  10  10  10  10  10  10  10  10  10  10  10  10  10  10  10\n",
      "  10  10  10  10  10  10  10  10  10  10  10  10  10  10  10  10  10  10\n",
      "  10  10  10  10  10  10  10  10  10  10  10  10  10  10  10  10  10  10\n",
      "  10  10  10  10  10  10  10  10  10  10  10  10  10  10  10  10  10  10\n",
      "  10  10  10  10  10  10  10  10  10  10  10  10  10  10  10  10  10  10\n",
      "  10  10  10  10  10  10  10  10  10  10  10  10  10  10  10  10  10  10\n",
      "  10  10  10  10  10  10  10  10  10  10  10  10  10  10  10  10  10  10\n",
      "  10  10  10  10  10  10  10  10  10  10  10  10  10  10  10  10  10  10\n",
      "  10  10  10  10  10  10  10  10  10  10  10  10  10  10  10  10]\n"
     ]
    }
   ],
   "source": [
    "#The number of following movie for each user\n",
    "each_user = np.sum(usr_following, axis=1)\n",
    "# print(each_user)\n",
    "\n",
    "print('Min number of followings:', np.min(each_user))\n",
    "print('Max number of followings:', np.max(each_user))\n",
    "\n",
    "asc = np.sort(each_user)\n",
    "# print(each_user)\n",
    "# print(asc)\n",
    "desc = np.flip(asc)\n",
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "print(desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Over 10: 1582\n",
      "Over 12: 937\n",
      "Over 14: 613\n",
      "Over 16: 440\n",
      "Over 18: 315\n",
      "Over 20: 229\n"
     ]
    }
   ],
   "source": [
    "print('Over 10:', np.sum(each_user >= 10))\n",
    "print('Over 12:', np.sum(each_user >= 12))\n",
    "print('Over 14:', np.sum(each_user >= 14))\n",
    "print('Over 16:', np.sum(each_user >= 16))\n",
    "print('Over 18:', np.sum(each_user >= 18))\n",
    "print('Over 20:', np.sum(each_user >= 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(937,)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "over12_idx = np.nonzero(each_user >= 12)[0]\n",
    "over12_idx.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1582 165\n"
     ]
    }
   ],
   "source": [
    "usr_nb = len(usr_following) # the number of users\n",
    "movie_nb = len(movie_genre)  # the number of movies\n",
    "\n",
    "print(usr_nb, movie_nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150 32\n"
     ]
    }
   ],
   "source": [
    "usr_test_amount = 150\n",
    "movie_test_amount = 32\n",
    "\n",
    "print(usr_test_amount, movie_test_amount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1582\n",
      "150 [13, 51, 54, 61, 65, 88, 93, 96, 114, 130]\n"
     ]
    }
   ],
   "source": [
    "usr_idx = [i for i in range(len(usr_following))]\n",
    "print(len(usr_idx))\n",
    "\n",
    "random.seed(42)\n",
    "test_idx = sorted(random.sample(usr_idx, usr_test_amount))\n",
    "print(len(test_idx), test_idx[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Training\n",
    "train_t = []\n",
    "train_f = []\n",
    "# Testing\n",
    "test_t = []\n",
    "test_f = []\n",
    "test_pos = -1\n",
    "\n",
    "for i in range(usr_nb):\n",
    "    \n",
    "    t_for_train = []\n",
    "    f_for_train = []\n",
    "    \n",
    "    if i not in test_idx: #if not in test id, just append it to true or false list\n",
    "        for j in range(movie_nb):\n",
    "            if usr_following[i][j] == 1:\n",
    "                t_for_train.append(j)\n",
    "            else:\n",
    "                f_for_train.append(j)\n",
    "                \n",
    "        train_t.append(t_for_train)\n",
    "        train_f.append(f_for_train)\n",
    "        \n",
    "    else: #if in test id, choose 2 true and other \n",
    "        test_pos += 1\n",
    "        temp_t = []\n",
    "        temp_f = []\n",
    "        \n",
    "        for j in range(movie_nb):\n",
    "            \n",
    "            if usr_following[i][j] == 1:\n",
    "                temp_t.append(j)\n",
    "            else:\n",
    "                temp_f.append(j)\n",
    "        \n",
    "        # random choose half true and half false for test \n",
    "        t_for_test = random.sample(temp_t, math.ceil(0.5*len(temp_t)))\n",
    "        f_for_test  = random.sample(temp_f, movie_test_amount-len(t_for_test))\n",
    "        \n",
    "        test_t.append(t_for_test)\n",
    "        test_f.append(f_for_test)\n",
    "        \n",
    "        #other for training\n",
    "        t_for_train = [item for item in temp_t if not item in t_for_test]\n",
    "        f_for_train = [item for item in temp_f if not item in f_for_test]\n",
    "        \n",
    "        train_t.append(t_for_train)\n",
    "        train_f.append(f_for_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of train_t: 1582\n",
      "The length of train_f: 1582\n",
      "The length of test_t: 150\n",
      "The length of test_f: 150\n"
     ]
    }
   ],
   "source": [
    "# train_t[i] 代表的是user i positive feedback\n",
    "print('The length of train_t:',len(train_t))\n",
    "print('The length of train_f:',len(train_f))\n",
    "print('The length of test_t:',len(test_t))\n",
    "print('The length of test_f:',len(test_f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: 14.139064475347661\n",
      "Testing: 7.1866666666666665\n"
     ]
    }
   ],
   "source": [
    "#average num of following for training user\n",
    "total_train = 0\n",
    "for t in train_t:\n",
    "    total_train += len(t)\n",
    "    \n",
    "avg = total_train / usr_nb\n",
    "print('Training:', avg)\n",
    "\n",
    "#average num of following for testing user\n",
    "total_test = 0\n",
    "for t in test_t:\n",
    "    total_test += len(t)\n",
    "avg = total_test / usr_test_amount\n",
    "print('Testing:', avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_auxilary = [i for i in range(movie_nb)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommendation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "SAVE_NAME = 'MRM_ALL_Embedding200_L2'\n",
    "LATENT_FOLDER = './latent_factor/MRM_ALL/Embedding200_L2/'\n",
    "newPath(LATENT_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128 4876 200\n"
     ]
    }
   ],
   "source": [
    "latent_dim = 128 # latent dims\n",
    "ft_dim = all_npy.shape[1] # feature dims\n",
    "embedding_dims = 200\n",
    "\n",
    "print(latent_dim, ft_dim, embedding_dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "user = tf.placeholder(tf.int32,shape=(1,))\n",
    "i = tf.placeholder(tf.int32, shape=(1,))\n",
    "j = tf.placeholder(tf.int32, shape=(1,))\n",
    "\n",
    "#多少個auxliary \n",
    "xf = tf.placeholder(tf.float32, shape=(None,ft_dim))\n",
    "l_id = tf.placeholder(tf.int32, shape=(None,))\n",
    "l_id_len = tf.placeholder(tf.int32,shape=(1,))\n",
    "r = tf.placeholder(tf.float32,shape=(None,))\n",
    "\n",
    "image_i = tf.placeholder(tf.float32, [1, ft_dim])\n",
    "image_j = tf.placeholder(tf.float32, [1, ft_dim])\n",
    "\n",
    "with tf.variable_scope(\"item_level\"):\n",
    "    user_latent = tf.get_variable(\"user_latent\", [usr_nb, latent_dim],\n",
    "                                  initializer=tf.random_normal_initializer(0,0.1,seed=3))\n",
    "    item_latent = tf.get_variable(\"item_latent\", [movie_nb, latent_dim],\n",
    "                                  initializer=tf.random_normal_initializer(0,0.1,seed=3)) \n",
    "    aux_item = tf.get_variable(\"aux_item\", [movie_nb, latent_dim],\n",
    "                               initializer=tf.random_normal_initializer(0,0.1,seed=3))\n",
    "    \n",
    "    W1 = tf.get_variable(\"W1\", [usr_nb, latent_dim],\n",
    "                         initializer=tf.contrib.layers.xavier_initializer())\n",
    "    Wu = tf.get_variable(\"Wu\", [latent_dim,latent_dim], \n",
    "                         initializer=tf.contrib.layers.xavier_initializer())\n",
    "    Wy = tf.get_variable(\"Wy\", [movie_nb, latent_dim, latent_dim],\n",
    "                         initializer=tf.contrib.layers.xavier_initializer())\n",
    "    Wa = tf.get_variable(\"Wa\", [latent_dim, latent_dim],\n",
    "                         initializer=tf.contrib.layers.xavier_initializer())\n",
    "    Wv = tf.get_variable(\"Wv\", [latent_dim, ft_dim],\n",
    "                         initializer=tf.contrib.layers.xavier_initializer())\n",
    "    \n",
    "    aux_new = tf.get_variable(\"aux_new\", [1, latent_dim], initializer=tf.constant_initializer(0.0))\n",
    "    ########## Error part, how to get auxisize dynamically\n",
    "    ####aux_size= tf.get_variable(name='aux_size', initializer=l_id.get_shape().as_list()[-1])\n",
    "    \n",
    "with tf.variable_scope('feature_level'):\n",
    "    embedding = tf.get_variable(\"embedding\", [embedding_dims,ft_dim],\n",
    "                                initializer=tf.contrib.layers.xavier_initializer())\n",
    "    Beta = tf.get_variable(\"beta\", [usr_nb, embedding_dims],\n",
    "                           initializer=tf.random_normal_initializer(0.00001,0.000001,seed=10))\n",
    "    \n",
    "#lookup the latent factors by user and id\n",
    "u = tf.nn.embedding_lookup(user_latent, user)\n",
    "vi = tf.nn.embedding_lookup(item_latent, i)\n",
    "vj = tf.nn.embedding_lookup(item_latent, j)\n",
    "\n",
    "w1 = tf.nn.embedding_lookup(W1, user) #(1*k)\n",
    "wu = Wu\n",
    "#wu = tf.squeeze(tf.nn.embedding_lookup(Wu, user)) #(k*k)\n",
    "wy = tf.squeeze(tf.nn.embedding_lookup(Wy, i)) #(k*k)\n",
    "wa = Wa\n",
    "#wa = tf.squeeze(tf.nn.embedding_lookup(Wa, user)) #(k*k)\n",
    "wv = Wv\n",
    "#wv = tf.squeeze(tf.nn.embedding_lookup(Wv, user)) #(k,l)\n",
    "\n",
    "beta = tf.nn.embedding_lookup(Beta, user) #user feature latent factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/tonylab/miniconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/ops/array_grad.py:425: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From <ipython-input-28-9e038058892d>:77: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "a_list = tf.Variable([])\n",
    "q = tf.constant(0)\n",
    "\n",
    "def att_cond(q,a_list):\n",
    "    return tf.less(q,l_id_len[0])\n",
    "\n",
    "def att_body(q,a_list):\n",
    "    xfi = tf.expand_dims(xf[q],0) #(1,ft_dim)\n",
    "    \n",
    "    a_list = tf.concat([a_list,[(tf.matmul( w1, tf.nn.relu( tf.matmul(wu, u, transpose_b=True) +\n",
    "        tf.matmul(wy, tf.expand_dims(tf.nn.embedding_lookup(item_latent,l_id[q]),0), transpose_b=True) +\n",
    "        tf.matmul(wa, tf.expand_dims(tf.nn.embedding_lookup(aux_item, l_id[q]),0), transpose_b=True) +\n",
    "        tf.matmul(wv, xfi, transpose_b=True)))[0][0])*r[q]]],0)\n",
    "    q += 1\n",
    "    return q,  a_list\n",
    "\n",
    "_, a_list = tf.while_loop(att_cond,att_body,[q,a_list],shape_invariants=[q.get_shape(),tf.TensorShape([None])])\n",
    "\n",
    "a_list_soft = tf.nn.softmax(a_list)\n",
    "\n",
    "\n",
    "aux_np = tf.expand_dims(tf.zeros(latent_dim),0)\n",
    "q = tf.constant(0)\n",
    "\n",
    "def sum_att_cond(q,aux_np):\n",
    "    return tf.less(q,l_id_len[0])\n",
    "\n",
    "def sum_att_body(q,aux_np):\n",
    "    #aux_np+=a_list_soft[q]*tf.expand_dims(tf.nn.embedding_lookup(aux_item, l_id[q]),0)\n",
    "    aux_np = tf.math.add_n([aux_np,a_list_soft[q]*tf.expand_dims(tf.nn.embedding_lookup(aux_item, l_id[q]),0)]) \n",
    "    q += 1\n",
    "    return q, aux_np\n",
    "\n",
    "_, aux_np = tf.while_loop(sum_att_cond, sum_att_body, [q,aux_np])\n",
    "\n",
    "aux_part = tf.matmul(aux_np, vi, transpose_b=True)\n",
    "#tf.print('aux attention:',aux_np)\n",
    "aux_np += u #user_latent factor + sum (alpha*auxilary)\n",
    "aux_new = tf.assign(aux_new,aux_np) #把aux_new 的 值變成aux_np\n",
    "\n",
    "\n",
    "latent_i_part = tf.matmul(aux_new, vi, transpose_b=True)\n",
    "feature_i_part = tf.matmul(beta,(tf.matmul(embedding,image_i, transpose_b=True)))\n",
    "latent_j_part = tf.matmul(aux_new, vj, transpose_b=True)\n",
    "feature_j_part = tf.matmul(beta,(tf.matmul(embedding,image_j, transpose_b=True)))\n",
    "only_aux_i_part = tf.matmul(aux_np, vi, transpose_b=True)\n",
    "only_aux_j_part = tf.matmul(aux_np, vj, transpose_b=True)\n",
    "\n",
    "#矩陣中對應函數各自相乘\n",
    "# ex: tf.matmul(thetav,(tf.matmul(embedding, image_i, transpose_b=True)))\n",
    "xui = tf.matmul(aux_new, vi, transpose_b=True)+ tf.matmul(beta,(tf.matmul(embedding,image_i, transpose_b=True)))\n",
    "xuj = tf.matmul(aux_new, vj, transpose_b=True)+ tf.matmul(beta,(tf.matmul(embedding,image_j, transpose_b=True)))\n",
    "\n",
    "xuij = tf.subtract(xui,xuj)\n",
    "\n",
    "norm_par = [tf.reduce_sum(tf.multiply(u, u)),tf.reduce_sum(tf.multiply(vi, vi)),tf.reduce_sum(tf.multiply(vj, vj)),\n",
    "           tf.reduce_sum(tf.multiply(w1, w1)),tf.reduce_sum(tf.multiply(wu, wu)),tf.reduce_sum(tf.multiply(wy, wy)),\n",
    "           tf.reduce_sum(tf.multiply(wa, wa)),tf.reduce_sum(tf.multiply(wv,wv)),tf.reduce_sum(tf.multiply(beta,beta))]\n",
    "l2_norm = tf.add_n([\n",
    "            0.0001 * tf.reduce_sum(tf.multiply(u, u)),\n",
    "            0.0001 * tf.reduce_sum(tf.multiply(vi, vi)),\n",
    "            0.0001 * tf.reduce_sum(tf.multiply(vj, vj)),\n",
    "  \n",
    "            0.0001 * tf.reduce_sum(tf.multiply(w1, w1)),\n",
    "            0.01 * tf.reduce_sum(tf.multiply(wu, wu)),\n",
    "            0.01 * tf.reduce_sum(tf.multiply(wy, wy)),\n",
    "            0.01 * tf.reduce_sum(tf.multiply(wa, wa)),\n",
    "            0.01 * tf.reduce_sum(tf.multiply(wv,wv)),\n",
    "            \n",
    "            0.001 * tf.reduce_sum(tf.multiply(beta,beta)),\n",
    "            0.01 * tf.reduce_sum(tf.multiply(embedding,embedding))\n",
    "            \n",
    "          ])\n",
    "\n",
    "loss = l2_norm - tf.log(tf.sigmoid(xuij)) # objective funtion\n",
    "train_op = tf.train.AdamOptimizer(learning_rate=0.0001).minimize(loss) #parameter optimize \n",
    "auc = tf.reduce_mean(tf.to_float(xuij > 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start time: Sat Mar  7 12:30:39 2020\n",
      "Iteration: 0\n",
      "total_loss          [[0.55990512]]\n",
      "train_auc:          0.8203773247496423\n",
      "\tCurrent time: Sat Mar  7 12:51:22 2020\n",
      "==================================================\n",
      "Iteration: 1\n",
      "total_loss          [[0.47979776]]\n",
      "train_auc:          0.8606983190271816\n",
      "\tCurrent time: Sat Mar  7 13:12:03 2020\n",
      "==================================================\n",
      "Iteration: 2\n",
      "total_loss          [[0.43251237]]\n",
      "train_auc:          0.8858369098712446\n",
      "\tCurrent time: Sat Mar  7 13:32:41 2020\n",
      "==================================================\n",
      "Iteration: 3\n",
      "total_loss          [[0.39716214]]\n",
      "train_auc:          0.9014440271816881\n",
      "\tCurrent time: Sat Mar  7 13:53:28 2020\n",
      "==================================================\n",
      "Iteration: 4\n",
      "total_loss          [[0.37242811]]\n",
      "train_auc:          0.9131840128755365\n",
      "\tCurrent time: Sat Mar  7 14:14:11 2020\n",
      "==================================================\n",
      "Total cost time: 6211.124539375305\n",
      "End time: Sat Mar  7 14:14:11 2020\n"
     ]
    }
   ],
   "source": [
    "print('Start time:', time.ctime())\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "loss_acc_list = []\n",
    "t0 = time.time()\n",
    "\n",
    "#use_true=init_list_of_objects(136)\n",
    "#use_test=init_list_of_objects(136)\n",
    "\n",
    "#train_pair_t=[] #positive feedback\n",
    "#train_pair_f=[] #negative feedback\n",
    "train_yes_id=[]\n",
    "\n",
    "for q in range(5):\n",
    "    print('Iteration:',q)\n",
    "    train_auc = 0\n",
    "    total_loss = 0\n",
    "    xuij_auc = 0\n",
    "    length = 0\n",
    "    \n",
    "    for z in range(usr_nb):\n",
    "        writeProgress('Progress:', z, usr_nb)\n",
    "        \"\"\"\n",
    "        yes 用來存放選擇到的YouTuber feature (for auxilary)\n",
    "        yesr 用來存放user對該YouTuber的喜好程度(user_category 跟 YouTuber_category的相似性)\n",
    "        r_3 用來存放user 對該YouTuber種類的偏好(取max)\n",
    "        \"\"\"\n",
    "        yes = []\n",
    "        yesr = []\n",
    "        \n",
    "#         #隨機選3個sample true's YouTuber\n",
    "#         sample = random.sample(train_t[z],len(train_t[z]))\n",
    "        #選全部的Positive\n",
    "        sample = random.sample(train_t[z],len(train_t[z]))\n",
    "        \n",
    "        train_yes_id.append(sample) #sample全部丟進去\n",
    "        \n",
    "        #sample=random.sample(train_t[z]+train_f[z],len(train_t[z])+len(train_f[z]))\n",
    "        \n",
    "        #change\n",
    "        r_3 = np.zeros(len(sample)) \n",
    "        alpha_history = []\n",
    "        a_list_history = []\n",
    "        U_history = []\n",
    "        Y_history = []\n",
    "        \n",
    "        #print(len(sample))\n",
    "        #check if all YouTuber are in train_t or train_f\n",
    "        #if len(train_t[z])+len(train_f[z]) != 88:\n",
    "            #print(z,len(train_t[z])+len(train_f[z]))\n",
    "         \n",
    "        for b in range(len(sample)):\n",
    "            yes.append(all_npy[sample[b]])\n",
    "            yesr.append(movie_genre[sample[b]] * usr_genre_norm[z])\n",
    "            #print('YouTuber_category ', YouTuber_category[sample[k]])\n",
    "            #print('User_category ',user_category_norm[z])\n",
    "        #print(len(yes))\n",
    "        \n",
    "        for b in range(len(yesr)):\n",
    "            r_3[b]=max(yesr[b])\n",
    "        #print('r_3:',r_3)\n",
    "        \n",
    "        yes = np.array(yes)\n",
    "        #print('user shape should be ',np.array([z]).shape)\n",
    "        #print('xf shape should be ',yes.shape)\n",
    "        #print('r shape should be ',np.array(r_3).shape)\n",
    "        #print('l_id shape should be ',np.array(sample).shape)\n",
    "        \n",
    "        #not_used_list = list(set(train_t[z]).difference(set(sample)))\n",
    "        \n",
    "        # positive \n",
    "        train_t_sample = random.sample(train_t[z],len(train_t[z]))\n",
    "        #print('number of positive feedback', len(train_t[z]))\n",
    "        # negative\n",
    "#         train_f_sample = random.sample(train_f[z],20)\n",
    "        \n",
    "        for ta in train_t_sample:\n",
    "            #print(ta,'--> positive feedback')\n",
    "            \n",
    "            pos = sample.index(ta)\n",
    "            \n",
    "            image_1=np.expand_dims(all_npy[ta],0)\n",
    "            train_f_sample = random.sample(train_f[z],10)\n",
    "            \n",
    "            for b in train_f_sample:\n",
    "                image_2=np.expand_dims(all_npy[b],0) #(1,2048)\n",
    "                #print('Image_2 shape',image_2.shape)\n",
    "            \n",
    "                #use_test[z].append(b)\n",
    "                _embedding,_a_list,r3,_auc, _loss,_=sess.run([embedding,a_list,a_list_soft,auc,loss,train_op], feed_dict={user: [z],\n",
    "                                        i: [ta], j: [b], xf: yes , l_id:sample, l_id_len:[len(sample)],r:r_3,\n",
    "                                        image_i:image_1,image_j:image_2})\n",
    "                #print(XUIJ)\n",
    "                #print('loss=',_loss)\n",
    "                #print('auc=',_auc)\n",
    "                \n",
    "                #print('after softmax:',r3)\n",
    "                #print('before softmax:',_a_list)\n",
    "                #print('embedding:',_embedding)\n",
    "                #print('---------------------------------------------------')\n",
    "                a_list_history.append(_a_list)\n",
    "                alpha_history.append(r3)\n",
    "                train_auc += _auc\n",
    "                total_loss += _loss\n",
    "                length += 1\n",
    "            #now1+=1\n",
    "        \n",
    "        np.save(LATENT_FOLDER + str(q) + '_' + str(z),_embedding)\n",
    "    \n",
    "    #print('mine:',xuij_auc/136)   \n",
    "    #print('a_list_soft:',r3)\n",
    "    print(\"{:<20}{}\".format('total_loss', total_loss/length))\n",
    "    print(\"{:<20}{}\".format('train_auc:', train_auc/length))\n",
    "    \n",
    "    loss_acc_list.append([total_loss/length, train_auc/length, time.time()-t0])\n",
    "    \n",
    "    print('\\tCurrent time:', time.ctime())\n",
    "    print('==================================================')\n",
    "    \n",
    "print('Total cost time:',time.time()-t0)\n",
    "\n",
    "print('End time:', time.ctime())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0\n",
      "loss= [[0.55990512]]\n",
      "acc= 0.8203773247496423\n",
      "==================================================\n",
      "Iteration: 1\n",
      "loss= [[0.47979776]]\n",
      "acc= 0.8606983190271816\n",
      "==================================================\n",
      "Iteration: 2\n",
      "loss= [[0.43251237]]\n",
      "acc= 0.8858369098712446\n",
      "==================================================\n",
      "Iteration: 3\n",
      "loss= [[0.39716214]]\n",
      "acc= 0.9014440271816881\n",
      "==================================================\n",
      "Iteration: 4\n",
      "loss= [[0.37242811]]\n",
      "acc= 0.9131840128755365\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(loss_acc_list)):\n",
    "    print('Iteration:',i)\n",
    "    print('loss=',loss_acc_list[i][0])\n",
    "    print('acc=',loss_acc_list[i][1])\n",
    "#     print('time=',loss_acc_list[i][2])\n",
    "    print('==================================================')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get latent factor and Each weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "U, Y, A, A1, Au, Ay, Aa, Av, E, B = sess.run([user_latent, item_latent, aux_item, \n",
    "                                              W1, Wu, Wy, Wa, Wv, embedding, Beta])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User latent shape:  (1582, 128)\n",
      "photo latent shape:  (165, 128)\n",
      "Auxilary latent shape:  (165, 128)\n",
      "W1 weight shape:  (1582, 128)\n",
      "Wu weight shape: (128, 128)\n",
      "Wy weight shape: (165, 128, 128)\n",
      "Wa weight shape: (128, 128)\n",
      "Wv weight shape: (128, 4876)\n",
      "Embedding shape: (200, 4876)\n",
      "Beta shape: (1582, 200)\n"
     ]
    }
   ],
   "source": [
    "print('User latent shape: ',U.shape)\n",
    "print('photo latent shape: ', Y.shape)\n",
    "print('Auxilary latent shape: ',A.shape)\n",
    "print('W1 weight shape: ',A1.shape)\n",
    "print('Wu weight shape:',Au.shape)\n",
    "print('Wy weight shape:', Ay.shape)\n",
    "print('Wa weight shape:', Aa.shape)\n",
    "print('Wv weight shape:', Av.shape)\n",
    "print('Embedding shape:', E.shape)\n",
    "print('Beta shape:',B.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.savez('./weight/' + SAVE_NAME + '.npz', \n",
    "         U=U, Y=Y, A=A, A1=A1, Wu=Au, Wy=Ay, Wa=Aa, Wv=Av, E=E, B=B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 13\n",
      "alpha:         [-1.86732083e-11  7.38520614e-09  1.25798476e-09 -1.00382685e-09\n",
      " -5.62949731e-10 -4.25034472e-09  1.29294239e-10]\n",
      "softmax alpha: [0.14285714 0.14285714 0.14285714 0.14285714 0.14285714 0.14285714\n",
      " 0.14285714]\n",
      "==================================================\n",
      "1 51\n",
      "alpha:         [ 2.46475607e-07  4.02408278e-08  4.61239915e-08  1.82962353e-08\n",
      "  2.06763161e-08 -2.46709941e-08]\n",
      "softmax alpha: [0.1666667  0.16666666 0.16666666 0.16666666 0.16666666 0.16666665]\n",
      "==================================================\n",
      "2 54\n",
      "alpha:         [9.67456655e-08 4.05519482e-08 1.11193161e-07 2.46216842e-08\n",
      " 8.57371319e-08 8.87956095e-09]\n",
      "softmax alpha: [0.16666667 0.16666666 0.16666667 0.16666666 0.16666667 0.16666666]\n",
      "==================================================\n",
      "3 61\n",
      "alpha:         [-1.44880152e-08 -3.92417516e-07 -1.99946223e-08 -9.54528901e-08\n",
      " -2.31607875e-07]\n",
      "softmax alpha: [0.20000003 0.19999995 0.20000003 0.20000001 0.19999998]\n",
      "==================================================\n",
      "4 65\n",
      "alpha:         [-1.35771098e-07 -1.35036570e-07  1.04240152e-08  3.04834580e-08\n",
      " -1.09828319e-08  3.48865239e-08]\n",
      "softmax alpha: [0.16666665 0.16666665 0.16666667 0.16666668 0.16666667 0.16666668]\n",
      "==================================================\n",
      "5 88\n",
      "alpha:         [-4.10646564e-12  2.75347839e-11  3.59380337e-11  3.88515042e-11\n",
      "  2.36980403e-12  5.94689666e-11  1.96011438e-11  8.58908850e-13\n",
      "  1.68641546e-11]\n",
      "softmax alpha: [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      " 0.11111111 0.11111111 0.11111111]\n",
      "==================================================\n",
      "6 93\n",
      "alpha:         [ 2.09348413e-09  8.12852421e-10  5.46312793e-09 -7.64559698e-10\n",
      " -1.75283887e-10  1.10034196e-12 -1.45047021e-09]\n",
      "softmax alpha: [0.14285714 0.14285714 0.14285714 0.14285714 0.14285714 0.14285714\n",
      " 0.14285714]\n",
      "==================================================\n",
      "7 96\n",
      "alpha:         [ 2.25684176e-07  2.80695601e-07 -4.46833950e-09  1.48313201e-07\n",
      " -3.62297490e-08]\n",
      "softmax alpha: [0.20000002 0.20000003 0.19999997 0.20000001 0.19999997]\n",
      "==================================================\n",
      "8 114\n",
      "alpha:         [1.53665324e-06 2.68087844e-07 3.14419647e-07 5.54781633e-08\n",
      " 4.39860128e-08]\n",
      "softmax alpha: [0.20000022 0.19999996 0.19999997 0.19999992 0.19999992]\n",
      "==================================================\n",
      "9 130\n",
      "alpha:         [ 9.14337133e-08 -1.11616568e-07  6.43922717e-10 -3.69912670e-07\n",
      "  4.66878131e-07]\n",
      "softmax alpha: [0.20000002 0.19999997 0.2        0.19999992 0.20000009]\n",
      "==================================================\n",
      "10 135\n",
      "alpha:         [ 2.79589801e-07 -8.41729702e-08 -2.65377801e-07 -1.29654197e-07\n",
      "  7.02773134e-08]\n",
      "softmax alpha: [0.20000006 0.19999999 0.19999995 0.19999998 0.20000002]\n",
      "==================================================\n",
      "11 142\n",
      "alpha:         [ 2.08484712e-08 -1.70934070e-07  5.00198647e-08  2.93649458e-08\n",
      " -1.26060178e-07  8.50508764e-08]\n",
      "softmax alpha: [0.16666667 0.16666664 0.16666668 0.16666667 0.16666665 0.16666668]\n",
      "==================================================\n",
      "12 146\n",
      "alpha:         [-8.77324611e-08  2.59671831e-07  2.03849531e-07 -3.11877194e-07\n",
      "  7.77862167e-09]\n",
      "softmax alpha: [0.19999998 0.20000005 0.20000004 0.19999993 0.2       ]\n",
      "==================================================\n",
      "13 161\n",
      "alpha:         [2.31355523e-07 3.84459225e-07 8.70467139e-09 3.45866791e-07\n",
      " 6.65596628e-08]\n",
      "softmax alpha: [0.2        0.20000004 0.19999996 0.20000003 0.19999997]\n",
      "==================================================\n",
      "14 163\n",
      "alpha:         [-2.65193571e-09 -8.26330637e-09  2.23690173e-08  9.58336513e-08\n",
      " -4.38940268e-08 -6.01892459e-08]\n",
      "softmax alpha: [0.16666667 0.16666667 0.16666667 0.16666668 0.16666666 0.16666666]\n",
      "==================================================\n",
      "15 178\n",
      "alpha:         [1.69138035e-07 4.83275192e-08 5.48169916e-07 3.80403149e-07\n",
      " 5.24622250e-07]\n",
      "softmax alpha: [0.19999997 0.19999994 0.20000004 0.20000001 0.20000004]\n",
      "==================================================\n",
      "16 186\n",
      "alpha:         [ 5.83905676e-08  1.61284746e-08 -9.82892152e-08  1.88977594e-07\n",
      "  4.29686918e-07  2.44705503e-07]\n",
      "softmax alpha: [0.16666665 0.16666665 0.16666663 0.16666667 0.16666671 0.16666668]\n",
      "==================================================\n",
      "17 189\n",
      "alpha:         [ 3.37709118e-07 -3.24570666e-07  2.43038676e-08  6.91471713e-07\n",
      "  2.10979307e-07]\n",
      "softmax alpha: [0.20000003 0.1999999  0.19999997 0.2000001  0.2       ]\n",
      "==================================================\n",
      "18 191\n",
      "alpha:         [-4.34969304e-08  4.42148986e-10  1.70620600e-08 -4.25163532e-08\n",
      " -3.01530687e-07]\n",
      "softmax alpha: [0.20000001 0.20000001 0.20000002 0.20000001 0.19999995]\n",
      "==================================================\n",
      "19 198\n",
      "alpha:         [ 3.40459125e-08 -1.14138506e-06  1.38208232e-07  4.28922500e-08\n",
      " -1.63317083e-07]\n",
      "softmax alpha: [0.20000005 0.19999982 0.20000007 0.20000005 0.20000001]\n",
      "==================================================\n",
      "20 206\n",
      "alpha:         [9.69627446e-09 2.30671988e-07 5.28178667e-07 1.46799391e-07\n",
      " 1.20963486e-07]\n",
      "softmax alpha: [0.19999996 0.2        0.20000006 0.19999999 0.19999998]\n",
      "==================================================\n",
      "21 209\n",
      "alpha:         [-1.05233266e-08  1.60483205e-07  3.03435969e-07  3.83470979e-07\n",
      "  4.61384285e-08  2.14430243e-08]\n",
      "softmax alpha: [0.16666664 0.16666667 0.16666669 0.16666671 0.16666665 0.16666665]\n",
      "==================================================\n",
      "22 224\n",
      "alpha:         [-9.39562700e-08 -5.56132177e-08  2.07566888e-08 -1.89630604e-07\n",
      "  1.81902539e-08  1.93985287e-08]\n",
      "softmax alpha: [0.16666666 0.16666667 0.16666668 0.16666664 0.16666668 0.16666668]\n",
      "==================================================\n",
      "23 228\n",
      "alpha:         [7.31863987e-08 6.39791884e-08 3.45559305e-07 8.06286383e-08\n",
      " 1.85261189e-08 3.17732897e-07]\n",
      "softmax alpha: [0.16666665 0.16666665 0.1666667  0.16666666 0.16666664 0.16666669]\n",
      "==================================================\n",
      "24 255\n",
      "alpha:         [-5.25601037e-10 -2.96530496e-09 -5.32781363e-10  9.17762945e-10\n",
      "  2.69366503e-10  5.92800196e-11  1.50248787e-09  7.79867538e-10]\n",
      "softmax alpha: [0.125 0.125 0.125 0.125 0.125 0.125 0.125 0.125]\n",
      "==================================================\n",
      "25 283\n",
      "alpha:         [-1.23471023e-09 -1.64446497e-09 -2.30507017e-10 -2.28289155e-09\n",
      " -5.67607257e-10 -3.16420807e-10 -4.57480624e-10]\n",
      "softmax alpha: [0.14285714 0.14285714 0.14285714 0.14285714 0.14285714 0.14285714\n",
      " 0.14285714]\n",
      "==================================================\n",
      "26 285\n",
      "alpha:         [ 1.56042271e-12  3.03145347e-12  3.59817784e-11 -1.99074180e-11\n",
      "  5.59059237e-11  1.06629286e-11  4.68337076e-12  5.00988071e-11\n",
      "  2.92754721e-11]\n",
      "softmax alpha: [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      " 0.11111111 0.11111111 0.11111111]\n",
      "==================================================\n",
      "27 292\n",
      "alpha:         [-1.12460745e-07 -2.74074183e-09 -1.72773026e-08 -5.09582417e-08\n",
      " -3.71562657e-08  5.00879451e-08]\n",
      "softmax alpha: [0.16666665 0.16666667 0.16666667 0.16666666 0.16666667 0.16666668]\n",
      "==================================================\n",
      "28 313\n",
      "alpha:         [-1.99419887e-08 -1.01502411e-07  3.85621137e-07 -1.92871688e-07\n",
      " -2.41026317e-08]\n",
      "softmax alpha: [0.19999999 0.19999998 0.20000008 0.19999996 0.19999999]\n",
      "==================================================\n",
      "29 318\n",
      "alpha:         [-2.78167693e-13 -5.58176474e-14 -4.29318077e-13 -1.16398146e-12\n",
      " -9.91295871e-13 -1.35362220e-13 -3.89011695e-13 -3.66272986e-14\n",
      "  1.57693803e-13 -9.20853511e-14  1.98107917e-14  7.65674402e-14]\n",
      "softmax alpha: [0.08333333 0.08333333 0.08333333 0.08333333 0.08333333 0.08333333\n",
      " 0.08333333 0.08333333 0.08333333 0.08333333 0.08333333 0.08333333]\n",
      "==================================================\n",
      "30 326\n",
      "alpha:         [ 5.89661283e-10  1.78088148e-09 -1.17461181e-09  8.38511667e-10\n",
      "  1.02745557e-09  3.10583829e-09  1.48122114e-09]\n",
      "softmax alpha: [0.14285714 0.14285714 0.14285714 0.14285714 0.14285714 0.14285714\n",
      " 0.14285714]\n",
      "==================================================\n",
      "31 327\n",
      "alpha:         [-4.36997052e-12 -4.24916413e-12 -5.45353257e-12  1.01354868e-11\n",
      " -2.40882324e-12  2.05367541e-11 -3.17461756e-12  1.91124500e-12\n",
      " -3.28855610e-11 -6.57783505e-13]\n",
      "softmax alpha: [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
      "==================================================\n",
      "32 333\n",
      "alpha:         [-5.30409510e-07  1.40654502e-07 -2.70599553e-08  2.60069808e-07\n",
      " -1.09757448e-07]\n",
      "softmax alpha: [0.1999999  0.20000004 0.20000001 0.20000006 0.19999999]\n",
      "==================================================\n",
      "33 334\n",
      "alpha:         [ 8.44900869e-08  2.03521551e-08 -6.13984171e-08  1.69908350e-07\n",
      "  1.44884281e-08  6.21163586e-08]\n",
      "softmax alpha: [0.16666667 0.16666666 0.16666665 0.16666669 0.16666666 0.16666667]\n",
      "==================================================\n",
      "34 350\n",
      "alpha:         [-7.97617887e-08  2.13990015e-07 -7.10208695e-09 -1.14426307e-07\n",
      " -1.66649975e-07 -1.78095941e-07]\n",
      "softmax alpha: [0.16666666 0.16666671 0.16666667 0.16666666 0.16666665 0.16666665]\n",
      "==================================================\n",
      "35 393\n",
      "alpha:         [-1.40468598e-18  4.46692779e-17  4.24971528e-17 -1.89998921e-18\n",
      "  7.57974996e-18  1.81612114e-17 -3.16195655e-17 -2.15161749e-17\n",
      " -6.81175843e-18  1.85736653e-18 -1.46426212e-18  2.09924219e-20\n",
      "  5.61145918e-18 -4.09643484e-17  9.89233055e-18]\n",
      "softmax alpha: [0.06666667 0.06666667 0.06666667 0.06666667 0.06666667 0.06666667\n",
      " 0.06666667 0.06666667 0.06666667 0.06666667 0.06666667 0.06666667\n",
      " 0.06666667 0.06666667 0.06666667]\n",
      "==================================================\n",
      "36 407\n",
      "alpha:         [-2.38925636e-07 -1.62853033e-07 -1.70355363e-08 -3.40722078e-08\n",
      " -1.04288686e-06]\n",
      "softmax alpha: [0.20000001 0.20000003 0.20000006 0.20000005 0.19999985]\n",
      "==================================================\n",
      "37 429\n",
      "alpha:         [-7.81128722e-07 -1.65559410e-07 -5.49342180e-07 -7.00180998e-08\n",
      " -1.75046302e-07]\n",
      "softmax alpha: [0.19999991 0.20000004 0.19999996 0.20000006 0.20000003]\n",
      "==================================================\n",
      "38 432\n",
      "alpha:         [-1.66326647e-09 -1.13649950e-07 -9.64620148e-08 -3.00463187e-07\n",
      " -6.71852320e-09 -1.83506918e-07]\n",
      "softmax alpha: [0.16666669 0.16666667 0.16666667 0.16666664 0.16666669 0.16666666]\n",
      "==================================================\n",
      "39 435\n",
      "alpha:         [ 1.00505585e-07  1.55583435e-07 -4.65543806e-07 -6.21884422e-07\n",
      " -4.86947664e-07]\n",
      "softmax alpha: [0.20000007 0.20000008 0.19999996 0.19999993 0.19999996]\n",
      "==================================================\n",
      "40 440\n",
      "alpha:         [-3.58014058e-16 -1.06591692e-15 -4.68176918e-16 -2.31597111e-16\n",
      "  1.10151853e-16 -1.02774679e-15  2.49572670e-18  5.34240085e-18\n",
      " -8.58906573e-17 -5.02873300e-17 -4.31449376e-16 -1.28406391e-17\n",
      " -3.62962216e-16]\n",
      "softmax alpha: [0.07692308 0.07692308 0.07692308 0.07692308 0.07692308 0.07692308\n",
      " 0.07692308 0.07692308 0.07692308 0.07692308 0.07692308 0.07692308\n",
      " 0.07692308]\n",
      "==================================================\n",
      "41 447\n",
      "alpha:         [-5.79476929e-08 -1.49568578e-07  2.93155431e-08  1.29038080e-08\n",
      "  2.20447677e-07 -3.82181058e-08]\n",
      "softmax alpha: [0.16666666 0.16666664 0.16666667 0.16666667 0.1666667  0.16666666]\n",
      "==================================================\n",
      "42 449\n",
      "alpha:         [ 1.70890208e-19 -2.75866373e-19 -1.53504634e-18  1.85056323e-18\n",
      " -8.60908840e-19  8.07583121e-19  9.72592390e-19  5.84565415e-19\n",
      " -2.46005355e-19 -5.35533688e-20  1.18905384e-19  5.23404673e-20\n",
      "  2.99097703e-18  4.16658337e-19  1.90298565e-18  9.83843685e-21\n",
      " -1.03552042e-18]\n",
      "softmax alpha: [0.05882353 0.05882353 0.05882353 0.05882353 0.05882353 0.05882353\n",
      " 0.05882353 0.05882353 0.05882353 0.05882353 0.05882353 0.05882353\n",
      " 0.05882353 0.05882353 0.05882353 0.05882353 0.05882353]\n",
      "==================================================\n",
      "43 451\n",
      "alpha:         [-1.45110767e-20 -6.13448179e-21 -2.11087737e-21 -4.98855025e-21\n",
      " -6.73856998e-22 -5.34244742e-21 -3.84125393e-22 -3.09506863e-21\n",
      "  2.91262700e-22 -4.14876638e-21 -4.98432912e-21 -9.24109786e-21\n",
      "  8.02802934e-21  1.48020450e-21 -4.44147404e-22 -1.10271078e-21\n",
      " -1.58149150e-21 -1.04376885e-21 -4.79374566e-22 -1.85093239e-21]\n",
      "softmax alpha: [0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05\n",
      " 0.05 0.05 0.05 0.05 0.05 0.05]\n",
      "==================================================\n",
      "44 457\n",
      "alpha:         [ 2.27752879e-07  1.49102877e-08 -2.27712799e-07 -3.25101785e-09\n",
      " -9.62323215e-08]\n",
      "softmax alpha: [0.20000005 0.20000001 0.19999996 0.2        0.19999998]\n",
      "==================================================\n",
      "45 466\n",
      "alpha:         [ 4.14880275e-08 -6.72732881e-08 -5.95570418e-08 -2.13694124e-08\n",
      " -3.23090103e-08 -1.77604161e-08]\n",
      "softmax alpha: [0.16666668 0.16666666 0.16666666 0.16666667 0.16666667 0.16666667]\n",
      "==================================================\n",
      "46 469\n",
      "alpha:         [-4.23042577e-10  2.99517617e-09  6.97416026e-10  4.14884719e-11\n",
      "  6.05746280e-11 -2.61664009e-09  3.86466534e-09 -9.38155194e-10]\n",
      "softmax alpha: [0.125 0.125 0.125 0.125 0.125 0.125 0.125 0.125]\n",
      "==================================================\n",
      "47 476\n",
      "alpha:         [-2.64568634e-07 -1.28436898e-07  1.75449598e-08  7.45661869e-08\n",
      "  1.67085783e-07]\n",
      "softmax alpha: [0.19999995 0.19999998 0.20000001 0.20000002 0.20000004]\n",
      "==================================================\n",
      "48 501\n",
      "alpha:         [ 3.20306455e-08 -8.36859891e-08 -1.52424220e-08 -1.57112457e-08\n",
      "  7.07754544e-08 -1.07575442e-08]\n",
      "softmax alpha: [0.16666667 0.16666665 0.16666666 0.16666666 0.16666668 0.16666667]\n",
      "==================================================\n",
      "49 505\n",
      "alpha:         [ 6.46258044e-09 -5.68272125e-10  1.02339203e-07  5.55398979e-08\n",
      " -5.50391795e-08 -1.94159603e-08]\n",
      "softmax alpha: [0.16666667 0.16666666 0.16666668 0.16666667 0.16666666 0.16666666]\n",
      "==================================================\n",
      "50 514\n",
      "alpha:         [3.20287410e-08 6.02999997e-10 9.92608700e-08 7.94659334e-07\n",
      " 1.94812149e-08]\n",
      "softmax alpha: [0.19999997 0.19999996 0.19999998 0.20000012 0.19999997]\n",
      "==================================================\n",
      "51 538\n",
      "alpha:         [-5.15795313e-07  1.43842455e-07 -6.43652002e-07 -3.42491233e-08\n",
      " -8.49076394e-08]\n",
      "softmax alpha: [0.19999994 0.20000007 0.19999992 0.20000004 0.20000003]\n",
      "==================================================\n",
      "52 541\n",
      "alpha:         [-1.63478860e-17 -1.13414357e-17 -1.53596560e-17 -9.33119136e-17\n",
      " -3.88255923e-17 -6.01728951e-17 -3.56828110e-17 -8.68496404e-17\n",
      " -7.08162220e-18 -1.17999898e-16 -1.89828409e-17 -8.09677747e-17\n",
      " -1.01554144e-16 -2.38456105e-17  1.38989904e-17 -1.28392471e-16]\n",
      "softmax alpha: [0.0625 0.0625 0.0625 0.0625 0.0625 0.0625 0.0625 0.0625 0.0625 0.0625\n",
      " 0.0625 0.0625 0.0625 0.0625 0.0625 0.0625]\n",
      "==================================================\n",
      "53 542\n",
      "alpha:         [ 5.26822686e-07 -1.38481171e-07 -1.19575647e-07 -1.03287079e-08\n",
      " -3.62415369e-08]\n",
      "softmax alpha: [0.2000001  0.19999996 0.19999997 0.19999999 0.19999998]\n",
      "==================================================\n",
      "54 546\n",
      "alpha:         [ 1.07757673e-10 -2.63718111e-09  2.59618517e-09 -1.38090856e-09\n",
      "  4.17332834e-10  6.59462867e-10  2.82543017e-10]\n",
      "softmax alpha: [0.14285714 0.14285714 0.14285714 0.14285714 0.14285714 0.14285714\n",
      " 0.14285714]\n",
      "==================================================\n",
      "55 548\n",
      "alpha:         [ 4.40418215e-08 -4.13507293e-07 -1.56876065e-07 -8.23385340e-09\n",
      " -3.91888326e-08 -4.25945323e-08]\n",
      "softmax alpha: [0.16666669 0.16666661 0.16666666 0.16666668 0.16666668 0.16666668]\n",
      "==================================================\n",
      "56 552\n",
      "alpha:         [-4.67935649e-08 -5.01396205e-08  2.62634794e-10  3.61850793e-08\n",
      "  1.24843239e-08]\n",
      "softmax alpha: [0.19999999 0.19999999 0.2        0.20000001 0.2       ]\n",
      "==================================================\n",
      "57 563\n",
      "alpha:         [ 1.32072095e-08  8.53046222e-08 -3.74960030e-09  3.47937164e-08\n",
      "  8.24650555e-08 -5.66461202e-08]\n",
      "softmax alpha: [0.16666666 0.16666668 0.16666666 0.16666667 0.16666668 0.16666665]\n",
      "==================================================\n",
      "58 569\n",
      "alpha:         [ 5.95602017e-08 -5.19586296e-08  6.53855766e-09  1.33499225e-07\n",
      "  8.75796517e-08  1.58105288e-08]\n",
      "softmax alpha: [0.16666667 0.16666665 0.16666666 0.16666668 0.16666667 0.16666666]\n",
      "==================================================\n",
      "59 592\n",
      "alpha:         [ 2.44087250e-10 -8.55419642e-11  1.50207656e-09  6.11644733e-11\n",
      "  3.86521462e-10 -1.46842775e-09  1.01604761e-09  5.15982659e-10]\n",
      "softmax alpha: [0.125 0.125 0.125 0.125 0.125 0.125 0.125 0.125]\n",
      "==================================================\n",
      "60 600\n",
      "alpha:         [ 1.02092577e-07 -3.86119731e-08  3.46070953e-08  1.90651424e-08\n",
      " -1.73537135e-07]\n",
      "softmax alpha: [0.20000002 0.19999999 0.20000001 0.20000001 0.19999997]\n",
      "==================================================\n",
      "61 644\n",
      "alpha:         [-1.04465383e-07 -2.29659312e-07 -1.20484799e-07 -1.14070701e-08\n",
      " -2.63962367e-07]\n",
      "softmax alpha: [0.20000001 0.19999998 0.20000001 0.20000003 0.19999998]\n",
      "==================================================\n",
      "62 646\n",
      "alpha:         [-3.74967990e-07 -3.07119320e-07 -8.16334573e-09 -4.38852979e-08\n",
      " -1.48921979e-08]\n",
      "softmax alpha: [0.19999995 0.19999997 0.20000003 0.20000002 0.20000003]\n",
      "==================================================\n",
      "63 664\n",
      "alpha:         [ 1.28846982e-16  1.58921206e-16 -4.04160957e-16 -1.33033938e-17\n",
      " -5.32380476e-18  1.82688375e-17 -6.88779356e-17 -5.43064335e-17\n",
      " -1.50190288e-16  4.25043456e-17  4.10492871e-16  2.16947344e-16\n",
      " -1.55973796e-16]\n",
      "softmax alpha: [0.07692308 0.07692308 0.07692308 0.07692308 0.07692308 0.07692308\n",
      " 0.07692308 0.07692308 0.07692308 0.07692308 0.07692308 0.07692308\n",
      " 0.07692308]\n",
      "==================================================\n",
      "64 689\n",
      "alpha:         [-7.19980598e-11 -5.36431549e-11 -3.54076150e-13 -3.68691949e-12\n",
      " -1.17795106e-11 -1.40337674e-11 -1.90104454e-11 -9.58439453e-12\n",
      " -9.77645323e-12]\n",
      "softmax alpha: [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      " 0.11111111 0.11111111 0.11111111]\n",
      "==================================================\n",
      "65 696\n",
      "alpha:         [-1.73123336e-11  3.08951077e-12 -3.38603072e-11 -2.44419067e-11\n",
      " -1.86542928e-11 -1.51625417e-11 -1.11191141e-11  3.11510830e-11\n",
      "  6.04155362e-12]\n",
      "softmax alpha: [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      " 0.11111111 0.11111111 0.11111111]\n",
      "==================================================\n",
      "66 704\n",
      "alpha:         [-2.68505934e-11  3.54404823e-11  1.79191281e-12  9.07663879e-13\n",
      "  4.91838136e-12  5.24116866e-12  2.63759751e-11  1.84671811e-11\n",
      "  4.75645191e-11 -5.27744177e-12]\n",
      "softmax alpha: [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
      "==================================================\n",
      "67 727\n",
      "alpha:         [-6.39055443e-08  1.14334574e-07  6.28105196e-07  3.94429107e-07\n",
      "  4.05001858e-07]\n",
      "softmax alpha: [0.19999993 0.19999996 0.20000007 0.20000002 0.20000002]\n",
      "==================================================\n",
      "68 735\n",
      "alpha:         [ 1.14364268e-07 -1.12912089e-07  7.44610896e-08  1.21118969e-07\n",
      "  1.82575908e-07]\n",
      "softmax alpha: [0.20000001 0.19999996 0.2        0.20000001 0.20000002]\n",
      "==================================================\n",
      "69 740\n",
      "alpha:         [-6.89277158e-09 -3.03670998e-08 -5.08382690e-07 -1.33135916e-06\n",
      " -3.12933882e-07]\n",
      "softmax alpha: [0.20000009 0.20000008 0.19999999 0.19999982 0.20000003]\n",
      "==================================================\n",
      "70 741\n",
      "alpha:         [ 2.54684280e-09 -9.12411102e-10  2.49417485e-10  2.30578127e-09\n",
      " -1.57912070e-09 -8.17170158e-10 -2.60124215e-09]\n",
      "softmax alpha: [0.14285714 0.14285714 0.14285714 0.14285714 0.14285714 0.14285714\n",
      " 0.14285714]\n",
      "==================================================\n",
      "71 747\n",
      "alpha:         [-1.77590117e-11  1.36910491e-11 -8.98061927e-13 -1.15427982e-11\n",
      "  2.74199614e-12  4.14258085e-12  3.00760509e-11  8.79082589e-12\n",
      "  4.90930366e-12]\n",
      "softmax alpha: [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      " 0.11111111 0.11111111 0.11111111]\n",
      "==================================================\n",
      "72 758\n",
      "alpha:         [-6.23612852e-12  3.34164866e-11 -1.26780543e-11  5.48690424e-14\n",
      "  3.61284149e-12 -2.17245105e-14  2.92341690e-11  1.24601314e-11\n",
      "  4.22221213e-12 -3.40474021e-12]\n",
      "softmax alpha: [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
      "==================================================\n",
      "73 775\n",
      "alpha:         [ 1.23529839e-07 -5.35721268e-09  2.27421938e-08  3.31523976e-07\n",
      " -2.73893140e-07]\n",
      "softmax alpha: [0.20000002 0.19999999 0.2        0.20000006 0.19999994]\n",
      "==================================================\n",
      "74 777\n",
      "alpha:         [-1.12447669e-09  3.67729562e-10 -4.06119315e-10 -1.41731645e-09\n",
      " -1.57737530e-10 -6.07990497e-11  3.26638494e-10 -1.52242850e-10]\n",
      "softmax alpha: [0.125 0.125 0.125 0.125 0.125 0.125 0.125 0.125]\n",
      "==================================================\n",
      "75 778\n",
      "alpha:         [-1.25027059e-08 -9.90642685e-08  1.09255536e-06 -1.63927997e-08\n",
      "  7.02445672e-08]\n",
      "softmax alpha: [0.19999996 0.19999994 0.20000018 0.19999996 0.19999997]\n",
      "==================================================\n",
      "76 781\n",
      "alpha:         [6.82062758e-08 2.72379796e-07 5.30371082e-08 1.46831329e-07\n",
      " 7.31217926e-07]\n",
      "softmax alpha: [0.19999996 0.2        0.19999996 0.19999998 0.2000001 ]\n",
      "==================================================\n",
      "77 788\n",
      "alpha:         [-2.35760174e-08  5.59176805e-08  9.90049205e-08  5.53625925e-09\n",
      "  2.76313589e-07 -1.25004706e-08]\n",
      "softmax alpha: [0.16666665 0.16666666 0.16666667 0.16666666 0.1666667  0.16666665]\n",
      "==================================================\n",
      "78 810\n",
      "alpha:         [ 2.78772661e-07 -2.90770375e-07  1.05267754e-06  3.85762625e-08\n",
      " -7.52815792e-08]\n",
      "softmax alpha: [0.20000002 0.1999999  0.20000017 0.19999997 0.19999994]\n",
      "==================================================\n",
      "79 817\n",
      "alpha:         [ 2.44055638e-18 -6.06054143e-17  5.08765707e-17 -3.37614993e-17\n",
      " -1.08510796e-16  2.95474821e-16  2.22591652e-16  2.80851591e-17\n",
      " -3.37538896e-16  2.30079446e-16  8.72884164e-17 -3.72309519e-17\n",
      " -8.24279041e-17]\n",
      "softmax alpha: [0.07692308 0.07692308 0.07692308 0.07692308 0.07692308 0.07692308\n",
      " 0.07692308 0.07692308 0.07692308 0.07692308 0.07692308 0.07692308\n",
      " 0.07692308]\n",
      "==================================================\n",
      "80 821\n",
      "alpha:         [ 8.17547134e-07  1.73679231e-07 -8.83580705e-08  3.02201195e-07\n",
      " -1.44995999e-07]\n",
      "softmax alpha: [0.20000012 0.19999999 0.19999994 0.20000002 0.19999993]\n",
      "==================================================\n",
      "81 859\n",
      "alpha:         [-3.50398945e-08 -4.09201406e-08 -2.41507921e-07  1.07696369e-08\n",
      " -2.88134890e-07]\n",
      "softmax alpha: [0.20000002 0.20000002 0.19999998 0.20000003 0.19999997]\n",
      "==================================================\n",
      "82 864\n",
      "alpha:         [ 8.01436576e-08 -2.48603326e-07  2.24529360e-08 -1.03225506e-07\n",
      " -5.84342071e-08 -1.44147423e-07]\n",
      "softmax alpha: [0.16666669 0.16666664 0.16666668 0.16666666 0.16666667 0.16666666]\n",
      "==================================================\n",
      "83 865\n",
      "alpha:         [ 2.90797838e-08  1.67537834e-08 -1.37305423e-08  3.66283243e-08\n",
      "  1.77459076e-07  1.33419094e-07]\n",
      "softmax alpha: [0.16666666 0.16666666 0.16666665 0.16666666 0.16666669 0.16666668]\n",
      "==================================================\n",
      "84 877\n",
      "alpha:         [ 2.47794986e-07  4.06294509e-07 -1.30261131e-07 -1.29065140e-08\n",
      "  6.20639487e-08]\n",
      "softmax alpha: [0.20000003 0.20000006 0.19999995 0.19999997 0.19999999]\n",
      "==================================================\n",
      "85 919\n",
      "alpha:         [ 1.26271511e-12 -3.05441061e-11  3.27438548e-11 -8.15550424e-12\n",
      " -2.37191826e-11 -4.24440091e-11 -5.65442713e-11 -3.83740293e-12\n",
      " -2.02705165e-11]\n",
      "softmax alpha: [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      " 0.11111111 0.11111111 0.11111111]\n",
      "==================================================\n",
      "86 928\n",
      "alpha:         [-8.40968357e-08  1.60086619e-07  1.99460501e-08  1.57881769e-07\n",
      " -2.09556331e-09]\n",
      "softmax alpha: [0.19999997 0.20000002 0.19999999 0.20000002 0.19999999]\n",
      "==================================================\n",
      "87 939\n",
      "alpha:         [-6.12260785e-07 -5.37208311e-07 -8.54998288e-08  3.07282352e-07\n",
      " -1.78608243e-08]\n",
      "softmax alpha: [0.19999992 0.19999993 0.20000002 0.2000001  0.20000003]\n",
      "==================================================\n",
      "88 940\n",
      "alpha:         [ 1.95149414e-07 -3.56568100e-07  2.99402882e-08 -1.54453444e-06\n",
      " -4.24324486e-09]\n",
      "softmax alpha: [0.20000011 0.2        0.20000007 0.19999976 0.20000007]\n",
      "==================================================\n",
      "89 946\n",
      "alpha:         [ 1.13466941e-11 -1.29308992e-11  1.77815341e-11 -1.30739685e-12\n",
      " -9.25441305e-12 -2.57916141e-11 -1.02739641e-11 -3.22041948e-11\n",
      "  8.09484070e-12]\n",
      "softmax alpha: [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      " 0.11111111 0.11111111 0.11111111]\n",
      "==================================================\n",
      "90 958\n",
      "alpha:         [ 6.30236399e-09  8.97726558e-10  1.60300990e-10  3.58580840e-10\n",
      " -2.72103673e-10 -5.14482439e-10 -8.26258186e-10]\n",
      "softmax alpha: [0.14285714 0.14285714 0.14285714 0.14285714 0.14285714 0.14285714\n",
      " 0.14285714]\n",
      "==================================================\n",
      "91 1010\n",
      "alpha:         [ 7.98138009e-09  1.47246139e-07  7.09357579e-08 -2.91330073e-08\n",
      "  8.97303809e-08  1.18493870e-07]\n",
      "softmax alpha: [0.16666666 0.16666668 0.16666667 0.16666665 0.16666667 0.16666668]\n",
      "==================================================\n",
      "92 1022\n",
      "alpha:         [-5.52657646e-11 -2.74804353e-10 -1.57512663e-10  6.98845898e-10\n",
      "  6.95888914e-10  1.11843090e-09 -3.19366730e-10 -1.02235879e-09]\n",
      "softmax alpha: [0.125 0.125 0.125 0.125 0.125 0.125 0.125 0.125]\n",
      "==================================================\n",
      "93 1034\n",
      "alpha:         [ 1.33476938e-09 -9.33676561e-11 -4.26318760e-09 -2.63117350e-09\n",
      " -1.31797292e-09  7.13030668e-10 -1.87326732e-10]\n",
      "softmax alpha: [0.14285714 0.14285714 0.14285714 0.14285714 0.14285714 0.14285714\n",
      " 0.14285714]\n",
      "==================================================\n",
      "94 1043\n",
      "alpha:         [-1.15488911e-10  2.22289118e-10 -1.97957635e-10  1.12934898e-10\n",
      "  3.71720074e-11  7.18249015e-10  2.30778825e-10]\n",
      "softmax alpha: [0.14285714 0.14285714 0.14285714 0.14285714 0.14285714 0.14285714\n",
      " 0.14285714]\n",
      "==================================================\n",
      "95 1083\n",
      "alpha:         [ 2.98145623e-09 -1.16453622e-10 -2.66964532e-09 -3.10610637e-10\n",
      " -1.59283484e-09 -2.36623831e-10 -2.57930416e-09  3.82496867e-11]\n",
      "softmax alpha: [0.125 0.125 0.125 0.125 0.125 0.125 0.125 0.125]\n",
      "==================================================\n",
      "96 1093\n",
      "alpha:         [ 2.19354735e-07  2.24543969e-07  8.00011450e-08  4.65661273e-08\n",
      " -8.26586812e-09  2.45881358e-08]\n",
      "softmax alpha: [0.16666669 0.16666669 0.16666666 0.16666666 0.16666665 0.16666665]\n",
      "==================================================\n",
      "97 1098\n",
      "alpha:         [5.73594784e-09 1.97939104e-09 1.68544549e-09 1.65612652e-09\n",
      " 1.38456321e-09 7.00421571e-09 5.42227119e-10]\n",
      "softmax alpha: [0.14285714 0.14285714 0.14285714 0.14285714 0.14285714 0.14285714\n",
      " 0.14285714]\n",
      "==================================================\n",
      "98 1103\n",
      "alpha:         [-6.11260758e-09 -3.44560053e-08  1.93964706e-08 -8.95620774e-08\n",
      " -3.08581040e-08 -2.66618891e-08]\n",
      "softmax alpha: [0.16666667 0.16666667 0.16666667 0.16666666 0.16666667 0.16666667]\n",
      "==================================================\n",
      "99 1116\n",
      "alpha:         [-8.08379888e-13 -2.63627018e-11 -9.13718759e-12  3.61934169e-11\n",
      " -1.21742433e-11  3.14414286e-12 -7.28933506e-12  1.82156154e-11\n",
      " -4.34403053e-12]\n",
      "softmax alpha: [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      " 0.11111111 0.11111111 0.11111111]\n",
      "==================================================\n",
      "100 1130\n",
      "alpha:         [-9.22915609e-07 -1.79706732e-07 -1.20985783e-07  5.53611711e-08\n",
      " -2.57662324e-07]\n",
      "softmax alpha: [0.19999987 0.20000002 0.20000003 0.20000007 0.20000001]\n",
      "==================================================\n",
      "101 1133\n",
      "alpha:         [ 3.68802620e-07 -4.40476592e-07 -4.48740496e-08 -5.96537495e-07\n",
      " -9.82508745e-08]\n",
      "softmax alpha: [0.20000011 0.19999994 0.20000002 0.19999991 0.20000001]\n",
      "==================================================\n",
      "102 1140\n",
      "alpha:         [ 1.67664406e-07 -3.17559367e-08  2.30711028e-07  3.39847749e-08\n",
      " -7.19260513e-08  4.68818279e-08]\n",
      "softmax alpha: [0.16666668 0.16666665 0.16666669 0.16666666 0.16666664 0.16666666]\n",
      "==================================================\n",
      "103 1149\n",
      "alpha:         [9.23560565e-07 1.78913442e-07 3.05303623e-09 5.54907502e-07\n",
      " 2.25513944e-07]\n",
      "softmax alpha: [0.20000011 0.19999996 0.19999993 0.20000004 0.19999997]\n",
      "==================================================\n",
      "104 1161\n",
      "alpha:         [5.20752064e-11 4.45811523e-10 1.03016670e-10 4.38501896e-10\n",
      " 2.82527143e-10 2.86811166e-09 4.42737554e-10]\n",
      "softmax alpha: [0.14285714 0.14285714 0.14285714 0.14285714 0.14285714 0.14285714\n",
      " 0.14285714]\n",
      "==================================================\n",
      "105 1182\n",
      "alpha:         [-1.39879721e-10 -5.33764965e-13 -7.55215265e-10 -1.07432897e-09\n",
      "  4.17705078e-09  2.12257979e-09  6.76016751e-10 -1.69916454e-09]\n",
      "softmax alpha: [0.125 0.125 0.125 0.125 0.125 0.125 0.125 0.125]\n",
      "==================================================\n",
      "106 1195\n",
      "alpha:         [-4.45109074e-08 -9.14988478e-08 -1.80913683e-07  6.85850596e-08\n",
      " -3.23442684e-07]\n",
      "softmax alpha: [0.20000001 0.2        0.19999999 0.20000004 0.19999996]\n",
      "==================================================\n",
      "107 1197\n",
      "alpha:         [ 2.05336238e-07 -9.70434029e-08 -3.73946546e-07  6.06289863e-09\n",
      " -5.21174365e-08]\n",
      "softmax alpha: [0.20000005 0.19999999 0.19999994 0.20000001 0.2       ]\n",
      "==================================================\n",
      "108 1206\n",
      "alpha:         [-8.68580495e-13 -4.80025384e-12  1.44518587e-11 -1.27009337e-11\n",
      "  2.90267814e-11 -1.70720781e-11 -5.03827133e-12 -8.57379714e-12\n",
      " -4.67703192e-12]\n",
      "softmax alpha: [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      " 0.11111111 0.11111111 0.11111111]\n",
      "==================================================\n",
      "109 1209\n",
      "alpha:         [-7.49974812e-08 -8.84668481e-08  1.85232795e-08 -2.20076227e-07\n",
      " -3.44581932e-07]\n",
      "softmax alpha: [0.20000001 0.20000001 0.20000003 0.19999998 0.19999996]\n",
      "==================================================\n",
      "110 1220\n",
      "alpha:         [-8.87586345e-17 -1.17975640e-15 -1.91218730e-16 -6.82771096e-17\n",
      " -4.62338832e-16 -1.91318549e-17 -1.10711458e-16 -1.78342093e-16\n",
      " -4.13994080e-16 -3.52767932e-17 -2.84976157e-16 -1.72667266e-16\n",
      " -1.62582363e-17]\n",
      "softmax alpha: [0.07692308 0.07692308 0.07692308 0.07692308 0.07692308 0.07692308\n",
      " 0.07692308 0.07692308 0.07692308 0.07692308 0.07692308 0.07692308\n",
      " 0.07692308]\n",
      "==================================================\n",
      "111 1221\n",
      "alpha:         [ 1.76616052e-08 -3.36351335e-08  1.42198479e-07  2.19386205e-08\n",
      " -1.05720119e-07 -5.14095565e-08]\n",
      "softmax alpha: [0.16666667 0.16666666 0.16666669 0.16666667 0.16666665 0.16666666]\n",
      "==================================================\n",
      "112 1232\n",
      "alpha:         [ 1.71494867e-08 -3.55449785e-08 -9.47615855e-07 -1.97016550e-07\n",
      " -1.64898767e-07]\n",
      "softmax alpha: [0.20000006 0.20000005 0.19999986 0.20000001 0.20000002]\n",
      "==================================================\n",
      "113 1236\n",
      "alpha:         [ 2.43453377e-08 -8.48105071e-08 -4.01394765e-08 -4.35373277e-08\n",
      " -2.28391486e-07]\n",
      "softmax alpha: [0.20000002 0.2        0.20000001 0.20000001 0.19999997]\n",
      "==================================================\n",
      "114 1247\n",
      "alpha:         [-2.38079168e-07 -8.46179573e-08 -2.87438523e-07  1.40363936e-08\n",
      " -2.48037084e-08 -2.20909400e-07]\n",
      "softmax alpha: [0.16666665 0.16666668 0.16666664 0.16666669 0.16666669 0.16666665]\n",
      "==================================================\n",
      "115 1266\n",
      "alpha:         [-9.16240482e-08 -1.19303497e-07 -2.18821737e-07 -2.76949955e-07\n",
      " -9.05458002e-08 -3.19874975e-09]\n",
      "softmax alpha: [0.16666667 0.16666667 0.16666665 0.16666664 0.16666667 0.16666669]\n",
      "==================================================\n",
      "116 1285\n",
      "alpha:         [ 1.05364493e-07  7.02606508e-09  4.10802616e-08 -1.07028685e-06\n",
      "  1.72898108e-07]\n",
      "softmax alpha: [0.20000005 0.20000003 0.20000004 0.19999982 0.20000006]\n",
      "==================================================\n",
      "117 1287\n",
      "alpha:         [ 4.52042469e-08  1.07467631e-07 -6.08201926e-08  1.64296936e-07\n",
      "  2.93606961e-07]\n",
      "softmax alpha: [0.19999999 0.2        0.19999997 0.20000001 0.20000004]\n",
      "==================================================\n",
      "118 1300\n",
      "alpha:         [ 3.09665419e-08 -2.24436713e-09  3.83046133e-08 -3.28424443e-08\n",
      " -7.90823402e-08  5.01009313e-09]\n",
      "softmax alpha: [0.16666667 0.16666667 0.16666667 0.16666666 0.16666665 0.16666667]\n",
      "==================================================\n",
      "119 1301\n",
      "alpha:         [-4.49693403e-17 -3.41089053e-16 -3.66407756e-16  4.86361866e-16\n",
      "  7.67363272e-17 -7.47058733e-16 -1.94524968e-16 -5.66825477e-16\n",
      " -4.51623904e-17 -1.33176575e-16 -1.94460381e-16  1.32893765e-16\n",
      " -1.07667998e-16]\n",
      "softmax alpha: [0.07692308 0.07692308 0.07692308 0.07692308 0.07692308 0.07692308\n",
      " 0.07692308 0.07692308 0.07692308 0.07692308 0.07692308 0.07692308\n",
      " 0.07692308]\n",
      "==================================================\n",
      "120 1309\n",
      "alpha:         [1.73508829e-07 3.58906763e-08 9.23980816e-09 5.23772340e-09\n",
      " 1.03521669e-07 1.94060465e-07]\n",
      "softmax alpha: [0.16666668 0.16666666 0.16666665 0.16666665 0.16666667 0.16666668]\n",
      "==================================================\n",
      "121 1310\n",
      "alpha:         [-1.43600960e-07 -4.18840821e-09 -4.36572939e-07  8.45125498e-08\n",
      " -3.72686364e-08]\n",
      "softmax alpha: [0.19999999 0.20000002 0.19999993 0.20000004 0.20000001]\n",
      "==================================================\n",
      "122 1316\n",
      "alpha:         [-3.89814132e-07 -6.84398323e-08  2.23347984e-08 -1.10023057e-07\n",
      " -1.43548623e-07 -2.83184951e-08]\n",
      "softmax alpha: [0.16666662 0.16666668 0.16666669 0.16666667 0.16666666 0.16666668]\n",
      "==================================================\n",
      "123 1327\n",
      "alpha:         [ 4.85485869e-10 -1.27555181e-09 -1.13531786e-10 -5.88024102e-10\n",
      " -1.41032314e-10  9.54851563e-10 -2.49249624e-10]\n",
      "softmax alpha: [0.14285714 0.14285714 0.14285714 0.14285714 0.14285714 0.14285714\n",
      " 0.14285714]\n",
      "==================================================\n",
      "124 1330\n",
      "alpha:         [-7.84073268e-08  2.77526719e-07 -5.43126420e-08 -6.41176395e-08\n",
      "  1.54137567e-09]\n",
      "softmax alpha: [0.19999998 0.20000005 0.19999999 0.19999998 0.2       ]\n",
      "==================================================\n",
      "125 1342\n",
      "alpha:         [1.16682262e-10 7.73547383e-10 9.48394945e-10 5.69156594e-10\n",
      " 1.26446660e-09 8.35213244e-11 7.89939260e-10 3.43886872e-10]\n",
      "softmax alpha: [0.125 0.125 0.125 0.125 0.125 0.125 0.125 0.125]\n",
      "==================================================\n",
      "126 1354\n",
      "alpha:         [-2.82555658e-19 -1.28851610e-17  9.13371973e-18  2.63959803e-18\n",
      " -7.25379088e-18 -1.45324959e-18 -2.39811108e-17 -2.88074302e-17\n",
      "  5.45607575e-18 -1.26728528e-17  5.40075200e-18 -9.93540284e-20\n",
      "  4.63302797e-18 -6.51694672e-18 -2.95682803e-17]\n",
      "softmax alpha: [0.06666667 0.06666667 0.06666667 0.06666667 0.06666667 0.06666667\n",
      " 0.06666667 0.06666667 0.06666667 0.06666667 0.06666667 0.06666667\n",
      " 0.06666667 0.06666667 0.06666667]\n",
      "==================================================\n",
      "127 1372\n",
      "alpha:         [ 7.05691313e-09  7.82939760e-08  1.34980158e-07  5.44246958e-09\n",
      " -2.94180006e-09 -9.96514026e-09]\n",
      "softmax alpha: [0.16666666 0.16666667 0.16666668 0.16666666 0.16666666 0.16666666]\n",
      "==================================================\n",
      "128 1385\n",
      "alpha:         [1.57392428e-08 2.05683754e-08 5.99529341e-07 2.93834100e-07\n",
      " 1.34345254e-07]\n",
      "softmax alpha: [0.19999996 0.19999996 0.20000008 0.20000002 0.19999998]\n",
      "==================================================\n",
      "129 1393\n",
      "alpha:         [ 9.09779740e-07 -3.41047111e-08  2.55365605e-07 -1.60841042e-07\n",
      "  7.72338808e-08]\n",
      "softmax alpha: [0.20000014 0.19999995 0.20000001 0.19999993 0.19999997]\n",
      "==================================================\n",
      "130 1399\n",
      "alpha:         [-1.01116734e-07 -9.38819980e-08  6.59499065e-08 -3.04249162e-08\n",
      "  2.00587459e-07]\n",
      "softmax alpha: [0.19999998 0.19999998 0.20000001 0.19999999 0.20000004]\n",
      "==================================================\n",
      "131 1402\n",
      "alpha:         [ 2.87857966e-09 -5.25268009e-10 -4.94744438e-08 -1.28452978e-08\n",
      "  2.92474385e-08  2.14672445e-08]\n",
      "softmax alpha: [0.16666667 0.16666667 0.16666666 0.16666666 0.16666667 0.16666667]\n",
      "==================================================\n",
      "132 1409\n",
      "alpha:         [-7.35773803e-08  1.01002161e-07 -1.18838047e-08 -3.11535905e-08\n",
      " -7.13117764e-08  1.52136352e-08]\n",
      "softmax alpha: [0.16666666 0.16666669 0.16666667 0.16666666 0.16666666 0.16666667]\n",
      "==================================================\n",
      "133 1429\n",
      "alpha:         [ 3.66933463e-08  1.10033300e-07  9.51694489e-08 -1.06677124e-07\n",
      "  3.75320564e-10  2.76523767e-08]\n",
      "softmax alpha: [0.16666667 0.16666668 0.16666668 0.16666664 0.16666666 0.16666667]\n",
      "==================================================\n",
      "134 1436\n",
      "alpha:         [-1.96763818e-10  3.16381352e-11  2.77840168e-10  1.82634228e-09\n",
      "  4.90418849e-12  8.17330904e-10  1.15868043e-10]\n",
      "softmax alpha: [0.14285714 0.14285714 0.14285714 0.14285714 0.14285714 0.14285714\n",
      " 0.14285714]\n",
      "==================================================\n",
      "135 1437\n",
      "alpha:         [ 1.84382622e-08  9.34526088e-08  1.51833081e-08 -4.36549790e-07\n",
      "  1.88652842e-07]\n",
      "softmax alpha: [0.20000001 0.20000002 0.20000001 0.19999992 0.20000004]\n",
      "==================================================\n",
      "136 1442\n",
      "alpha:         [ 7.68484611e-08  3.19324926e-09 -1.04539206e-07  3.01501475e-08\n",
      " -1.92718256e-09]\n",
      "softmax alpha: [0.20000002 0.2        0.19999998 0.20000001 0.2       ]\n",
      "==================================================\n",
      "137 1466\n",
      "alpha:         [ 6.42071009e-08  3.55263780e-07  1.27296399e-07 -4.51579584e-08\n",
      " -4.87671249e-08]\n",
      "softmax alpha: [0.19999999 0.20000005 0.20000001 0.19999997 0.19999997]\n",
      "==================================================\n",
      "138 1470\n",
      "alpha:         [1.29090205e-07 7.71751666e-07 3.79330482e-09 2.02453491e-08\n",
      " 4.36740650e-07]\n",
      "softmax alpha: [0.19999997 0.2000001  0.19999995 0.19999995 0.20000003]\n",
      "==================================================\n",
      "139 1493\n",
      "alpha:         [ 3.58200179e-08 -1.38821229e-06 -8.92565566e-07  2.05728927e-08\n",
      "  3.84161991e-08]\n",
      "softmax alpha: [0.20000009 0.19999981 0.19999991 0.20000009 0.2000001 ]\n",
      "==================================================\n",
      "140 1494\n",
      "alpha:         [-3.30740256e-08 -6.35822670e-08 -3.74752816e-08 -2.57781693e-08\n",
      " -9.72725106e-07]\n",
      "softmax alpha: [0.20000004 0.20000003 0.20000004 0.20000004 0.19999985]\n",
      "==================================================\n",
      "141 1508\n",
      "alpha:         [-1.49647025e-11 -1.47934703e-11 -8.22640169e-12 -1.52194302e-11\n",
      " -1.67535059e-11 -2.70852779e-13 -1.65252423e-11 -1.37625560e-11\n",
      " -4.05481681e-11  2.90466705e-12]\n",
      "softmax alpha: [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
      "==================================================\n",
      "142 1516\n",
      "alpha:         [-1.05036757e-09  7.91194575e-10 -2.19339135e-10  1.24593358e-09\n",
      "  1.20631638e-09  6.69731949e-10 -9.46458704e-10 -4.01007908e-10]\n",
      "softmax alpha: [0.125 0.125 0.125 0.125 0.125 0.125 0.125 0.125]\n",
      "==================================================\n",
      "143 1518\n",
      "alpha:         [ 2.01204576e-08  1.53958489e-08 -3.12929554e-08  1.16681842e-07\n",
      "  6.44509482e-08  1.81931263e-07]\n",
      "softmax alpha: [0.16666666 0.16666666 0.16666665 0.16666668 0.16666667 0.16666669]\n",
      "==================================================\n",
      "144 1525\n",
      "alpha:         [ 1.31160767e-11  1.47447694e-11  4.03614618e-11 -3.00723043e-12\n",
      "  1.49825308e-12  3.10486192e-11  9.84326736e-12 -6.11343648e-12\n",
      "  3.54799667e-11]\n",
      "softmax alpha: [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      " 0.11111111 0.11111111 0.11111111]\n",
      "==================================================\n",
      "145 1529\n",
      "alpha:         [ 3.16886929e-07  2.79861947e-08 -2.45092017e-07 -5.27854195e-08\n",
      "  4.64322692e-07]\n",
      "softmax alpha: [0.20000004 0.19999999 0.19999993 0.19999997 0.20000007]\n",
      "==================================================\n",
      "146 1547\n",
      "alpha:         [-1.34469612e-14  1.62583534e-14  2.20712232e-15  6.80642318e-14\n",
      " -2.43470377e-14 -2.88069496e-14 -5.32967384e-15 -7.97732581e-14\n",
      " -4.26643789e-15  4.96649570e-15 -1.51882450e-14]\n",
      "softmax alpha: [0.09090909 0.09090909 0.09090909 0.09090909 0.09090909 0.09090909\n",
      " 0.09090909 0.09090909 0.09090909 0.09090909 0.09090909]\n",
      "==================================================\n",
      "147 1554\n",
      "alpha:         [ 1.57694889e-07  4.72899989e-07 -2.46550490e-07  1.51499699e-07\n",
      "  2.02715023e-07]\n",
      "softmax alpha: [0.2        0.20000007 0.19999992 0.2        0.20000001]\n",
      "==================================================\n",
      "148 1563\n",
      "alpha:         [-1.66429522e-11 -7.26925987e-11 -3.53715796e-11 -3.74728789e-11\n",
      "  2.17147168e-12 -4.03938225e-11 -1.01226654e-11 -1.01500546e-10\n",
      " -1.69115176e-12]\n",
      "softmax alpha: [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      " 0.11111111 0.11111111 0.11111111]\n",
      "==================================================\n",
      "149 1573\n",
      "alpha:         [-2.11800650e-08  7.05030591e-08  4.26558364e-08  1.02627110e-07\n",
      " -1.48038634e-08  2.29147488e-07]\n",
      "softmax alpha: [0.16666665 0.16666667 0.16666666 0.16666667 0.16666665 0.16666669]\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "usr_test_amount = 150\n",
    "movie_test_amount = 32\n",
    "'''\n",
    "\n",
    "#with Embedding\n",
    "result = np.zeros((usr_test_amount, movie_nb))\n",
    "RS = np.zeros((usr_test_amount, movie_nb))\n",
    "#test_idx --> Test 的 index\n",
    "\n",
    "test_yes_id = []\n",
    "\n",
    "for s in range(usr_test_amount):\n",
    "    print(s, test_idx[s])\n",
    "\n",
    "    yes = []\n",
    "    sample = random.sample(train_t[test_idx[s]],len(train_t[test_idx[s]])) #從training part 的positive feedback 取出YouTuber 當成Auxilary\n",
    "    #sample=result_yes_id[now]\n",
    "    test_yes_id.append(sample)\n",
    "    alpha = np.zeros([len(sample)])\n",
    "    \n",
    "    for a in range(len(sample)):\n",
    "        r = np.max(movie_genre[sample[a]] * usr_genre_norm[test_idx[s]]) #sample a 的category vec *user_category vec\n",
    "        #print(test_idx[s])\n",
    "        #print(np.dot(Au[test_idx[s]],np.expand_dims(U[test_idx[s]],0)))\n",
    "        alpha[a] = np.dot(A1[test_idx[s]],(relu(np.dot(Au,np.expand_dims(U[test_idx[s]],0).T) +\n",
    "                                                np.dot(Ay[sample[a]],np.expand_dims(Y[sample[a]],0).T) +\n",
    "                                                np.dot(Aa,np.expand_dims(A[sample[a]],0).T) +\n",
    "                                                np.dot(Av,np.expand_dims(all_npy[sample[a]],0).T)))) * r\n",
    "    mul = np.zeros((1,latent_dim))\n",
    "    \n",
    "    print(\"{:<15}{}\".format('alpha:', alpha))\n",
    "    print(\"{:<15}{}\".format('softmax alpha:', softmax(alpha)))\n",
    "    print('==================================================')\n",
    "    \n",
    "    for i in range(len(sample)):\n",
    "        mul += softmax(alpha)[i] * A[sample[i]] #attention alpha*Ai part \n",
    "    new_mul = mul + U[test_idx[s]]  #(U+auxilary)\n",
    "    \n",
    "    for k in range(movie_nb):\n",
    "        result[s][k] = np.dot(new_mul,Y[k].T) #(U+auxilary)*photo latent factor\n",
    "        RS[s][k] = np.dot(new_mul,Y[k].T) + np.dot(B[test_idx[s]], np.dot(E, all_npy[k].T))\n",
    "        \n",
    "#print(RS[s])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#取出test的資料\n",
    "testRS = np.zeros((usr_test_amount, movie_test_amount)) #shape 150*20\n",
    "target = np.zeros((usr_test_amount, movie_test_amount))\n",
    "#test_t 是true的\n",
    "#test_f 是false的\n",
    "        \n",
    "for z in range(usr_test_amount):\n",
    "    user_id = test_idx[z]\n",
    "    #positive target YouTuber list\n",
    "    youtube_t = test_t[z] \n",
    "    #not target YouTuber list\n",
    "    youtube_f = test_f[z]\n",
    "    \n",
    "    #前兩個放target的RS\n",
    "    for i in range(len(youtube_t)):\n",
    "        testRS[z][i] = RS[z][youtube_t[i]]\n",
    "        target[z][i] = 1\n",
    "        \n",
    "    for i in range(len(youtube_f)):\n",
    "        testRS[z][i+len(youtube_t)] = RS[z][youtube_f[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 32) (150, 32)\n"
     ]
    }
   ],
   "source": [
    "print(target.shape, testRS.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of positive data in testing: 1078.0\n",
      "total testing data: 4800\n"
     ]
    }
   ],
   "source": [
    "sumtarget = 0\n",
    "for i in range(len(target)):\n",
    "    #print(np.sum(target[i]))\n",
    "    sumtarget += np.sum(target[i])\n",
    "print('num of positive data in testing:',sumtarget)\n",
    "print('total testing data:', usr_test_amount * movie_test_amount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def topN(sortlist,n):\n",
    "    topList = []\n",
    "    for i in range(n):\n",
    "        topList.append(sortlist.index(max(sortlist)))\n",
    "        #print(max(sortlist))\n",
    "        #print(sortlist.index(max(sortlist)))\n",
    "        sortlist[sortlist.index(max(sortlist))] = -1000000000\n",
    "    return topList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "count_0_all = []\n",
    "for i in range(len(testRS)):\n",
    "    top_0 = topN(list(testRS[i]),int(np.sum(target[i])))\n",
    "    count_0_all.append(top_0)\n",
    "    #print(top_0)\n",
    "\n",
    "acc_0 = 0\n",
    "total = 0\n",
    "for i in range(len(count_0_all)):\n",
    "    for j in range(len(count_0_all[i])):\n",
    "        #print(int(np.sum(target[i])))\n",
    "        total+=int(np.sum(target[i]))\n",
    "        if count_0_all[i][j] < int(np.sum(target[i])): #代表是0或1 (也就是target)\n",
    "            acc_0 += 1\n",
    "avg_acc = acc_0/100\n",
    "#print('avg_accuarcy for count_0:',avg_acc)\n",
    "#print(acc_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "507"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8834"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Top n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def F1_score(prec,rec):\n",
    "    f1 = (2*prec*rec)/(prec+rec)\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "correct = 0\n",
    "for i in range(len(testRS)):\n",
    "    top_0 = topN(list(testRS[i]),1) #取一個\n",
    "    count_0_all.append(top_0)\n",
    "    #print(np.sum(target[i]))\n",
    "    #print(top_0)\n",
    "    if top_0[0] < int(np.sum(target[i])):\n",
    "        correct += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prec  0.52 recall  0.07235621521335807\n",
      "F1_score: 0.12703583061889248\n"
     ]
    }
   ],
   "source": [
    "top1_prec = correct/len(testRS)\n",
    "top1_recall = correct/(sumtarget)\n",
    "print('prec ',top1_prec,'recall ',top1_recall)\n",
    "print('F1_score:',F1_score(top1_prec,top1_recall))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "correct = 0\n",
    "for i in range(len(testRS)):\n",
    "    top_3 = topN(list(testRS[i]),3) #取一個\n",
    "    count_0_all.append(top_3)\n",
    "    #print(top_3)\n",
    "    for j in range(len(top_3)):\n",
    "        if top_3[j] < int(np.sum(target[i])):\n",
    "            correct += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prec  0.4444444444444444 recall  0.18552875695732837\n",
      "F1_score: 0.2617801047120419\n"
     ]
    }
   ],
   "source": [
    "top3_prec = correct/(len(testRS)*3)\n",
    "top3_recall = correct/(sumtarget)\n",
    "print('prec ',top3_prec,'recall ',top3_recall)\n",
    "print('F1_score:',F1_score(top3_prec,top3_recall))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "correct = 0\n",
    "for i in range(len(testRS)):\n",
    "    top_5 = topN(list(testRS[i]),5) #取一個\n",
    "    count_0_all.append(top_5)\n",
    "    #print(top_5)\n",
    "    for j in range(len(top_5)):\n",
    "        if top_5[j] < int(np.sum(target[i])):\n",
    "            correct += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prec  0.46266666666666667 recall  0.32189239332096475\n",
      "F1_score: 0.37964989059080967\n"
     ]
    }
   ],
   "source": [
    "top5_prec = correct/(len(testRS)*5)\n",
    "top5_recall = correct/(sumtarget)\n",
    "print('prec ',top5_prec,'recall ',top5_recall)\n",
    "print('F1_score:',F1_score(top5_prec,top5_recall))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pre_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "usr_test_amount = 150\n",
    "movie_test_amount = 16\n",
    "'''\n",
    "all_sort = []\n",
    "pre_matrix = np.zeros(shape=(usr_test_amount, movie_test_amount))\n",
    "for i in range(usr_test_amount):\n",
    "    top_5 = topN(list(testRS[i]),5) #取一個\n",
    "    #print(top_5)\n",
    "    all_sort.append(topN(list(testRS[i]),len(testRS[i])))\n",
    "    for j in range(len(top_5)):\n",
    "        pre_matrix[i][top_5[j]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 32) (150, 32)\n"
     ]
    }
   ],
   "source": [
    "print(pre_matrix.shape, target.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NDCG\n",
    "* https://daiwk.github.io/posts/nlp-ndcg.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Ideal DCG，理想状况下的DCG。也就是说，相关性完全由高到低排序时算出的DCG：\n",
    "\n",
    "def IDCG(ideal_list): #ideal_list example = [1,1,1,1,1,0,0,....]\n",
    "    idcg = 0\n",
    "    for i in range(len(ideal_list)):\n",
    "        #print((2**true_list[i]-1),math.log2(i+2))\n",
    "        idcg += (2**ideal_list[i]-1)/math.log2(i+2)\n",
    "    #print('idcg',idcg)\n",
    "    return idcg\n",
    "\n",
    "def DCG(prec_list): #找出前n名的[1,1,1,0,...]\n",
    "    dcg = 0\n",
    "    for i in range(len(prec_list)):\n",
    "        dcg += (2**prec_list[i]-1)/math.log2(i+2)\n",
    "    #print('dcg',dcg)\n",
    "    return dcg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NDCG: 0.4692476981758553\n"
     ]
    }
   ],
   "source": [
    "total_ndcg = 0\n",
    "num_ndcg = 5\n",
    "for m in range(usr_test_amount):\n",
    "    idcg = IDCG([1]*num_ndcg)\n",
    "    pre_list = []\n",
    "    for s in all_sort[m][:num_ndcg]:\n",
    "        #print(s)\n",
    "        #print(target[m][s])\n",
    "        pre_list.append(target[m][s])\n",
    "    dcg = DCG(pre_list)\n",
    "    ndcg = dcg/idcg\n",
    "    #print(ndcg)\n",
    "    total_ndcg += ndcg\n",
    "avg_ndcg = total_ndcg/usr_test_amount\n",
    "print('NDCG:',avg_ndcg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import average_precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP: 0.33385484988867326\n"
     ]
    }
   ],
   "source": [
    "total_prec = 0\n",
    "for u in range(usr_test_amount):\n",
    "    y_true = target[u]\n",
    "    y_scores = pre_matrix[u]\n",
    "    total_prec += average_precision_score(y_true, y_scores)\n",
    "    \n",
    "MAP = total_prec/usr_test_amount\n",
    "\n",
    "print('MAP:', MAP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
