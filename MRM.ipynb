{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import math\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Helper function\n",
    "def newPath(path):\n",
    "    if not os.path.isdir(path):\n",
    "        os.mkdir(path)\n",
    "        \n",
    "def writeProgress(msg, count, total):\n",
    "    sys.stdout.write(msg + \"{:.2%}\\r\".format(count/total))\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return np.maximum(0,x)  \n",
    "\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x)\n",
    "    softmax_x = exp_x / np.sum(exp_x)\n",
    "    return softmax_x \n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All features: (165, 4876)\n",
      "Movie genre: (165, 20)\n",
      "User following: (1582, 165)\n",
      "User genre: (1582, 20)\n"
     ]
    }
   ],
   "source": [
    "all_npy = np.load('./npy/all_4876.npy')\n",
    "movie_genre = np.load('./npy/movie_genre.npy')\n",
    "usr_following = np.load('./npy/user_followings.npy')\n",
    "usr_genre = np.load('./npy/user_genre.npy')\n",
    "\n",
    "print('All features:', all_npy.shape)\n",
    "print('Movie genre:', movie_genre.shape)\n",
    "print('User following:', usr_following.shape)\n",
    "print('User genre:', usr_genre.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1582 165\n",
      "150 32\n"
     ]
    }
   ],
   "source": [
    "usr_nb = len(usr_following) # the number of users\n",
    "movie_nb = len(movie_genre)  # the number of movies\n",
    "\n",
    "print(usr_nb, movie_nb)\n",
    "\n",
    "usr_test_amount = 150\n",
    "movie_test_amount = 32\n",
    "\n",
    "print(usr_test_amount, movie_test_amount)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize usr_genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1582, 20)\n"
     ]
    }
   ],
   "source": [
    "usr_genre_norm = np.zeros(usr_genre.shape)\n",
    "for i in range(len(usr_genre)):\n",
    "    usr_genre_norm[i] = usr_genre[i]/np.max(usr_genre[i])\n",
    "print(usr_genre_norm.shape)\n",
    "# print('Before:', usr_genre)\n",
    "# print('After:', usr_genre_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training & testing split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min number of followers: 1\n",
      "Max number of followers: 520\n",
      "Avg of followers: 142.0969696969697\n",
      "The num of followers over 5: 163\n"
     ]
    }
   ],
   "source": [
    "#The number of followers for each movie\n",
    "moive_followers = np.sum(usr_following, axis=0)\n",
    "# print(moive_followers)\n",
    "\n",
    "print('Min number of followers:', np.min(moive_followers))\n",
    "print('Max number of followers:', np.max(moive_followers))\n",
    "print('Avg of followers:', np.mean(moive_followers))\n",
    "\n",
    "asc = np.sort(moive_followers)\n",
    "# print(asc)\n",
    "desc = np.flip(asc)\n",
    "# print(desc)\n",
    "\n",
    "over5 = 0\n",
    "for num in moive_followers:\n",
    "    if num >= 5:\n",
    "        over5 += 1\n",
    "print('The num of followers over 5:', over5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min number of followings: 10\n",
      "Max number of followings: 133\n",
      "Avg of followers: 14.820480404551201\n"
     ]
    }
   ],
   "source": [
    "#The number of following movie for each user\n",
    "each_user = np.sum(usr_following, axis=1)\n",
    "# print(each_user)\n",
    "\n",
    "print('Min number of followings:', np.min(each_user))\n",
    "print('Max number of followings:', np.max(each_user))\n",
    "print('Avg of followers:', np.mean(each_user))\n",
    "\n",
    "asc = np.sort(each_user)\n",
    "# print(each_user)\n",
    "# print(asc)\n",
    "desc = np.flip(asc)\n",
    "# print(desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Over 10: 1582\n",
      "Over 12: 937\n",
      "Over 14: 613\n",
      "Over 16: 440\n",
      "Over 18: 315\n",
      "Over 20: 229\n"
     ]
    }
   ],
   "source": [
    "print('Over 10:', np.sum(each_user >= 10))\n",
    "print('Over 12:', np.sum(each_user >= 12))\n",
    "print('Over 14:', np.sum(each_user >= 14))\n",
    "print('Over 16:', np.sum(each_user >= 16))\n",
    "print('Over 18:', np.sum(each_user >= 18))\n",
    "print('Over 20:', np.sum(each_user >= 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# over12_idx = np.nonzero(each_user >= 12)[0]\n",
    "# over12_idx.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1582\n",
      "150 [13, 51, 54, 61, 65, 88, 93, 96, 114, 130]\n"
     ]
    }
   ],
   "source": [
    "usr_idx = [i for i in range(len(usr_following))]\n",
    "print(len(usr_idx))\n",
    "\n",
    "random.seed(42)\n",
    "test_idx = sorted(random.sample(usr_idx, usr_test_amount))\n",
    "print(len(test_idx), test_idx[:10]) # 150 [13, 51, 54, 61, 65, 88, 93, 96, 114, 130]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_t = []\n",
    "train_f = []\n",
    "test_t = []\n",
    "test_f = []\n",
    "test_pos = -1\n",
    "\n",
    "for i in range(usr_nb):\n",
    "    \n",
    "    t_for_train = []\n",
    "    f_for_train = []\n",
    "    \n",
    "    if i not in test_idx: #if not in test id, just append it to true or false list\n",
    "        for j in range(movie_nb):\n",
    "            if usr_following[i][j] == 1:\n",
    "                t_for_train.append(j)\n",
    "            else:\n",
    "                f_for_train.append(j)\n",
    "                \n",
    "        train_t.append(t_for_train)\n",
    "        train_f.append(f_for_train)\n",
    "        \n",
    "    else: #if in test id, choose 2 true and other \n",
    "        test_pos += 1\n",
    "        temp_t = []\n",
    "        temp_f = []\n",
    "        \n",
    "        for j in range(movie_nb):\n",
    "            \n",
    "            if usr_following[i][j] == 1:\n",
    "                temp_t.append(j)\n",
    "            else:\n",
    "                temp_f.append(j)\n",
    "        \n",
    "        # random choose half true and half false for test \n",
    "        t_for_test = random.sample(temp_t, math.ceil(0.5*len(temp_t)))\n",
    "        f_for_test  = random.sample(temp_f, movie_test_amount-len(t_for_test))\n",
    "        test_t.append(t_for_test)\n",
    "        test_f.append(f_for_test)\n",
    "        \n",
    "        #the others for training\n",
    "        t_for_train = [item for item in temp_t if not item in t_for_test]\n",
    "        f_for_train = [item for item in temp_f if not item in f_for_test]\n",
    "        train_t.append(t_for_train)\n",
    "        train_f.append(f_for_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of train_t: 1582\n",
      "The length of train_f: 1582\n",
      "The length of test_t: 150\n",
      "The length of test_f: 150\n"
     ]
    }
   ],
   "source": [
    "# train_t[i] 代表的是user i positive feedback\n",
    "print('The length of train_t:',len(train_t))\n",
    "print('The length of train_f:',len(train_f))\n",
    "print('The length of test_t:',len(test_t))\n",
    "print('The length of test_f:',len(test_f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: 14.139064475347661\n",
      "Testing: 7.1866666666666665\n"
     ]
    }
   ],
   "source": [
    "#average num of following for training user\n",
    "total_train = 0\n",
    "for t in train_t:\n",
    "    total_train += len(t)\n",
    "avg = total_train / usr_nb\n",
    "print('Training:', avg)\n",
    "\n",
    "#average num of following for testing user\n",
    "total_test = 0\n",
    "for t in test_t:\n",
    "    total_test += len(t)\n",
    "avg = total_test / usr_test_amount\n",
    "print('Testing:', avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_auxilary = [i for i in range(movie_nb)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommendation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "SAVE_NAME = 'MRM_ALL_Embedding200_L2_'\n",
    "# LATENT_FOLDER = './latent_factor/MRM_ALL/Embedding200_L2_/'\n",
    "# newPath(LATENT_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128 4876 200\n"
     ]
    }
   ],
   "source": [
    "latent_dim = 128 # latent dims\n",
    "ft_dim = all_npy.shape[1] # feature dims\n",
    "embedding_dims = 200\n",
    "\n",
    "print(latent_dim, ft_dim, embedding_dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/tonylab/miniconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "user = tf.placeholder(tf.int32,shape=(1,))\n",
    "i = tf.placeholder(tf.int32, shape=(1,))\n",
    "j = tf.placeholder(tf.int32, shape=(1,))\n",
    "\n",
    "#多少個auxliary \n",
    "xf = tf.placeholder(tf.float32, shape=(None, ft_dim))\n",
    "l_id = tf.placeholder(tf.int32, shape=(None,))\n",
    "l_id_len = tf.placeholder(tf.int32,shape=(1,))\n",
    "r = tf.placeholder(tf.float32,shape=(None,))\n",
    "positive_id = tf.placeholder(tf.int32, shape=(None,))\n",
    "positive_len = tf.placeholder(tf.int32,shape=(1,))\n",
    "\n",
    "image_i = tf.placeholder(tf.float32, [1, ft_dim])\n",
    "image_j = tf.placeholder(tf.float32, [1, ft_dim])\n",
    "\n",
    "with tf.variable_scope(\"item_level\"):\n",
    "    user_latent = tf.get_variable(\"user_latent\", [usr_nb, latent_dim],\n",
    "                                  initializer=tf.random_normal_initializer(0,0.1,seed=3))\n",
    "    item_latent = tf.get_variable(\"item_latent\", [movie_nb, latent_dim],\n",
    "                                  initializer=tf.random_normal_initializer(0,0.1,seed=3)) \n",
    "    aux_item = tf.get_variable(\"aux_item\", [movie_nb, latent_dim],\n",
    "                               initializer=tf.random_normal_initializer(0,0.1,seed=3))\n",
    "    \n",
    "#     W1 = tf.get_variable(\"W1\", [usr_nb, movie_nb, latent_dim], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    Wu = tf.get_variable(\"Wu\", [usr_nb, movie_nb, latent_dim], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    Wy = tf.get_variable(\"Wy\", [usr_nb, movie_nb, latent_dim], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    Wa = tf.get_variable(\"Wa\", [usr_nb, movie_nb, latent_dim], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    Wv = tf.get_variable(\"Wv\", [usr_nb, movie_nb, embedding_dims], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    Wve =  tf.get_variable(\"Wve\", [embedding_dims, ft_dim], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    \n",
    "    aux_new = tf.get_variable(\"aux_new\", [1, latent_dim], initializer=tf.constant_initializer(0.0))\n",
    "\n",
    "with tf.variable_scope('feature_level'):\n",
    "    embedding = tf.get_variable(\"embedding\", [embedding_dims,ft_dim],\n",
    "                                initializer=tf.contrib.layers.xavier_initializer())\n",
    "    Beta = tf.get_variable(\"beta\", [usr_nb, embedding_dims],\n",
    "                           initializer=tf.random_normal_initializer(0.01, 0.001, seed=10))\n",
    "    \n",
    "#lookup the latent factors by user and id\n",
    "u = tf.nn.embedding_lookup(user_latent, user)\n",
    "vi = tf.nn.embedding_lookup(item_latent, i)\n",
    "vj = tf.nn.embedding_lookup(item_latent, j)\n",
    "\n",
    "# w1 = tf.nn.embedding_lookup(W1, user)\n",
    "wu = tf.squeeze(tf.nn.embedding_lookup(Wu, user))\n",
    "wy = tf.squeeze(tf.nn.embedding_lookup(Wy, user))\n",
    "wa = tf.squeeze(tf.nn.embedding_lookup(Wa, user))\n",
    "wv = tf.squeeze(tf.nn.embedding_lookup(Wv, user))\n",
    "\n",
    "beta = tf.nn.embedding_lookup(Beta, user) #user feature latent factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/tonylab/miniconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/ops/array_grad.py:425: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From <ipython-input-20-5bd7555a0d01>:89: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "a_list = tf.Variable([])\n",
    "q = tf.constant(0)\n",
    "\n",
    "def att_cond(q,a_list):\n",
    "    return tf.less(q,l_id_len[0])\n",
    "\n",
    "def att_body(q,a_list):\n",
    "    xfi = tf.expand_dims(xf[q],0) #(1,l)\n",
    "    wuui = tf.expand_dims(tf.nn.embedding_lookup(wu,l_id[q]),0) #取該YOUTUBER那欄(1,K)\n",
    "    wyui = tf.expand_dims(tf.nn.embedding_lookup(wy,l_id[q]),0) #取該YOUTUBER那欄(1,K)\n",
    "    waui = tf.expand_dims(tf.nn.embedding_lookup(wa,l_id[q]),0) #取該YOUTUBER那欄(1,K)\n",
    "    wvui = tf.expand_dims(tf.nn.embedding_lookup(wv,l_id[q]),0) #取該YOUTUBER那欄(1,K)\n",
    "    \n",
    "    a_list = tf.concat([a_list,[(tf.nn.relu(tf.matmul(wuui, u, transpose_b=True) +\n",
    "                                            tf.matmul(wyui, tf.expand_dims(tf.nn.embedding_lookup(item_latent,l_id[q]),0), transpose_b=True) +\n",
    "                                            tf.matmul(waui, tf.expand_dims(tf.nn.embedding_lookup(aux_item, l_id[q]),0), transpose_b=True) +\n",
    "                                            tf.matmul(wvui,tf.matmul(Wve,xfi, transpose_b=True)))[0][0])*r[q]]],0)\n",
    "    q += 1\n",
    "    return q, a_list\n",
    "\n",
    "_, a_list = tf.while_loop(att_cond,att_body,[q,a_list],shape_invariants=[q.get_shape(),tf.TensorShape([None])])\n",
    "\n",
    "a_list_smooth = tf.add(a_list,0.0000000001)\n",
    "a_list_soft = tf.divide(a_list_smooth,tf.reduce_sum(a_list_smooth, 0)) #without softmax\n",
    "\n",
    "norm_par = [wu,wy,wa,wv]\n",
    "\n",
    "\n",
    "wuui = tf.expand_dims(tf.nn.embedding_lookup(wu,l_id[-1]),0)\n",
    "wyui = tf.expand_dims(tf.nn.embedding_lookup(wy,l_id[-1]),0)\n",
    "waui = tf.expand_dims(tf.nn.embedding_lookup(wa,l_id[-1]),0)\n",
    "wvui = tf.expand_dims(tf.nn.embedding_lookup(wv,l_id[-1]),0)\n",
    "wu_be_relu = tf.matmul(wuui, u, transpose_b=True)\n",
    "wy_be_relu = tf.matmul(wyui, tf.expand_dims(tf.nn.embedding_lookup(item_latent,l_id[-1]),0), transpose_b=True)\n",
    "wa_be_relu = tf.matmul(waui, tf.expand_dims(tf.nn.embedding_lookup(aux_item, l_id[-1]),0), transpose_b=True)\n",
    "wv_be_relu = tf.matmul(wvui, tf.matmul(Wve,tf.expand_dims(xf[-1],0), transpose_b=True))\n",
    "\n",
    "last_be_relu = [wu_be_relu,wy_be_relu,wa_be_relu,wv_be_relu]\n",
    "\n",
    "aux_np = tf.expand_dims(tf.zeros(latent_dim),0)\n",
    "q = tf.constant(0)\n",
    "\n",
    "def sum_att_cond(q,aux_np):\n",
    "    return tf.less(q,l_id_len[0])\n",
    "\n",
    "def sum_att_body(q,aux_np):\n",
    "    aux_np = tf.math.add_n([aux_np,a_list_soft[q]*tf.expand_dims(tf.nn.embedding_lookup(aux_item, l_id[q]),0)]) \n",
    "    q += 1\n",
    "    return q, aux_np\n",
    "\n",
    "_, aux_np = tf.while_loop(sum_att_cond, sum_att_body, [q,aux_np])\n",
    "\n",
    "aux_part = tf.matmul(aux_np, vi, transpose_b=True)\n",
    "#tf.print('aux attention:',aux_np)\n",
    "aux_np += u #user_latent factor + sum (alpha*auxilary)\n",
    "aux_new = tf.assign(aux_new,aux_np) #把aux_new 的 值變成aux_np\n",
    "\n",
    "latent_i_part = tf.matmul(aux_new, vi, transpose_b=True)\n",
    "feature_i_part = tf.matmul(beta,(tf.matmul(embedding,image_i, transpose_b=True)))\n",
    "latent_j_part = tf.matmul(aux_new, vj, transpose_b=True)\n",
    "feature_j_part = tf.matmul(beta,(tf.matmul(embedding,image_j, transpose_b=True)))\n",
    "only_aux_i_part = tf.matmul(aux_np, vi, transpose_b=True)\n",
    "only_aux_j_part = tf.matmul(aux_np, vj, transpose_b=True)\n",
    "\n",
    "#矩陣中對應函數各自相乘\n",
    "# ex: tf.matmul(thetav,(tf.matmul(embedding, image_i, transpose_b=True)))\n",
    "xui = tf.matmul(aux_new, vi, transpose_b=True)+ tf.matmul(beta,(tf.matmul(embedding,image_i, transpose_b=True)))\n",
    "xuj = tf.matmul(aux_new, vj, transpose_b=True)+ tf.matmul(beta,(tf.matmul(embedding,image_j, transpose_b=True)))\n",
    "\n",
    "xuij = tf.subtract(xui,xuj)\n",
    "\n",
    "l2_norm = tf.add_n([\n",
    "            0.0001 * tf.reduce_sum(tf.multiply(u, u)),\n",
    "            0.0001 * tf.reduce_sum(tf.multiply(vi, vi)),\n",
    "            0.0001 * tf.reduce_sum(tf.multiply(vj, vj)),\n",
    "  \n",
    "            0.01 * tf.reduce_sum(tf.multiply(wu, wu)),\n",
    "            0.01 * tf.reduce_sum(tf.multiply(wy, wy)),\n",
    "            0.01 * tf.reduce_sum(tf.multiply(wa, wa)),\n",
    "            0.01 * tf.reduce_sum(tf.multiply(wv,wv)),\n",
    "            \n",
    "            0.001 * tf.reduce_sum(tf.multiply(beta,beta)),\n",
    "            0.01 * tf.reduce_sum(tf.multiply(embedding,embedding))\n",
    "            \n",
    "          ])\n",
    "\n",
    "loss = l2_norm - tf.log(tf.sigmoid(xuij)) # objective funtion\n",
    "train_op = tf.train.AdamOptimizer(learning_rate=0.0001).minimize(loss) #parameter optimize \n",
    "auc = tf.reduce_mean(tf.to_float(xuij > 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start time: Sun Mar  8 23:16:22 2020\n",
      "Iteration: 0\n",
      "total_loss          [[0.5738897]]\n",
      "train_auc:          0.8062097639484979\n",
      "\tCurrent time: Mon Mar  9 01:57:32 2020  sec\n",
      "==================================================\n",
      "Iteration: 1\n",
      "total_loss          [[0.50706275]]\n",
      "train_auc:          0.8459451001430616\n",
      "\tCurrent time: Mon Mar  9 04:33:44 2020  sec\n",
      "==================================================\n",
      "Iteration: 2\n",
      "total_loss          [[0.47694844]]\n",
      "train_auc:          0.8656473533619456\n",
      "\tCurrent time: Mon Mar  9 07:08:57 2020  sec\n",
      "==================================================\n",
      "Iteration: 3\n",
      "total_loss          [[0.45689947]]\n",
      "train_auc:          0.8761355507868384\n",
      "\tCurrent time: Mon Mar  9 09:44:21 2020  sec\n",
      "==================================================\n",
      "Iteration: 4\n",
      "total_loss          [[0.44103655]]\n",
      "train_auc:          0.8845985336194564\n",
      "\tCurrent time: Mon Mar  9 12:21:18 2020  sec\n",
      "==================================================\n",
      "Iteration: 5\n",
      "total_loss          [[0.4338476]]\n",
      "train_auc:          0.8889887339055794\n",
      "\tCurrent time: Mon Mar  9 14:58:23 2020  sec\n",
      "==================================================\n",
      "Iteration: 6\n",
      "total_loss          [[0.42840271]]\n",
      "train_auc:          0.8921092632331903\n",
      "\tCurrent time: Mon Mar  9 17:31:08 2020  sec\n",
      "==================================================\n",
      "Iteration: 7\n",
      "total_loss          [[0.42332689]]\n",
      "train_auc:          0.8960345135908441\n",
      "\tCurrent time: Mon Mar  9 19:49:19 2020  sec\n",
      "==================================================\n",
      "Iteration: 8\n",
      "total_loss          [[0.42093095]]\n",
      "train_auc:          0.8981402002861231\n",
      "\tCurrent time: Mon Mar  9 22:11:12 2020  sec\n",
      "==================================================\n",
      "Iteration: 9\n",
      "total_loss          [[0.41788154]]\n",
      "train_auc:          0.9000223533619456\n",
      "\tCurrent time: Tue Mar 10 00:31:26 2020  sec\n",
      "==================================================\n",
      "Total cost time: 90900.3617913723  sec\n",
      "End time: Tue Mar 10 00:31:26 2020\n"
     ]
    }
   ],
   "source": [
    "print('Start time:', time.ctime())\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "loss_acc_list = []\n",
    "t0 = time.time()\n",
    "\n",
    "train_yes_id=[]\n",
    "\n",
    "for q in range(10):\n",
    "    print('Iteration:',q)\n",
    "    train_auc = 0\n",
    "    total_loss = 0\n",
    "    xuij_auc = 0\n",
    "    length = 0\n",
    "    \n",
    "    for z in range(usr_nb):\n",
    "        writeProgress('Progress:', z, usr_nb)\n",
    "        \"\"\"\n",
    "        yes 用來存放選擇到的YouTuber feature (for auxilary)\n",
    "        yesr 用來存放user對該YouTuber的喜好程度(user_category 跟 YouTuber_category的相似性)\n",
    "        r_3 用來存放user 對該YouTuber種類的偏好(取max)\n",
    "        \"\"\"\n",
    "        yes = []\n",
    "        yesr = []\n",
    "        \n",
    "        #選全部的Positive\n",
    "        sample = random.sample(train_t[z],len(train_t[z]))\n",
    "#         #選全部的電影\n",
    "#         sample = all_auxilary\n",
    "        \n",
    "        #change\n",
    "        r_3 = np.zeros(len(sample))\n",
    "         \n",
    "        for b in range(len(sample)):\n",
    "            yes.append(all_npy[sample[b]])\n",
    "            yesr.append(movie_genre[sample[b]] * usr_genre_norm[z])\n",
    "        \n",
    "        for b in range(len(yesr)):\n",
    "            r_3[b]=max(yesr[b])\n",
    "        #print('r_3:',r_3)\n",
    "        \n",
    "        yes = np.array(yes)\n",
    "        \n",
    "        # positive \n",
    "        train_t_sample = random.sample(train_t[z],len(train_t[z]))\n",
    "        #print('number of positive feedback', len(train_t[z]))\n",
    "        # negative\n",
    "#         train_f_sample = random.sample(train_f[z],20)\n",
    "        \n",
    "        for ta in train_t_sample:\n",
    "            #print(ta,'--> positive feedback')\n",
    "            \n",
    "            pos = sample.index(ta)\n",
    "            \n",
    "            image_1=np.expand_dims(all_npy[ta],0)\n",
    "            train_f_sample = random.sample(train_f[z],10)\n",
    "            \n",
    "            for b in train_f_sample:\n",
    "                image_2 = np.expand_dims(all_npy[b],0)\n",
    "                \n",
    "                _last_be_relu, _norm_par, _a_list, r3, _auc, _loss, _ = sess.run(\n",
    "                    [last_be_relu, norm_par, a_list_smooth, a_list_soft, auc, loss, train_op], \n",
    "                    feed_dict={user: [z], i: [ta], j: [b], xf: yes , \n",
    "                               l_id:sample, l_id_len:[len(sample)],\n",
    "                               positive_id: train_t[z], positive_len:[len(train_t[z])],\n",
    "                               r:r_3, image_i:image_1, image_j:image_2})\n",
    "                \n",
    "                train_auc += _auc\n",
    "                total_loss += _loss\n",
    "                length += 1\n",
    "        \n",
    "#         np.save(LATENT_FOLDER + str(q) + '_' + str(z),_embedding)\n",
    "    \n",
    "    #print('mine:',xuij_auc/136)   \n",
    "    #print('a_list_soft:',r3)\n",
    "    print(\"{:<20}{}\".format('total_loss', total_loss/length))\n",
    "    print(\"{:<20}{}\".format('train_auc:', train_auc/length))\n",
    "    \n",
    "    loss_acc_list.append([total_loss/length, train_auc/length, time.time()-t0])\n",
    "    \n",
    "    print('\\tCurrent time:', time.ctime(), ' sec')\n",
    "    print('==================================================')\n",
    "    \n",
    "print('Total cost time:',time.time()-t0, ' sec')\n",
    "\n",
    "print('End time:', time.ctime())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0\n",
      "loss= [[0.5738897]]\n",
      "acc= 0.8062097639484979\n",
      "==================================================\n",
      "Iteration: 1\n",
      "loss= [[0.50706275]]\n",
      "acc= 0.8459451001430616\n",
      "==================================================\n",
      "Iteration: 2\n",
      "loss= [[0.47694844]]\n",
      "acc= 0.8656473533619456\n",
      "==================================================\n",
      "Iteration: 3\n",
      "loss= [[0.45689947]]\n",
      "acc= 0.8761355507868384\n",
      "==================================================\n",
      "Iteration: 4\n",
      "loss= [[0.44103655]]\n",
      "acc= 0.8845985336194564\n",
      "==================================================\n",
      "Iteration: 5\n",
      "loss= [[0.4338476]]\n",
      "acc= 0.8889887339055794\n",
      "==================================================\n",
      "Iteration: 6\n",
      "loss= [[0.42840271]]\n",
      "acc= 0.8921092632331903\n",
      "==================================================\n",
      "Iteration: 7\n",
      "loss= [[0.42332689]]\n",
      "acc= 0.8960345135908441\n",
      "==================================================\n",
      "Iteration: 8\n",
      "loss= [[0.42093095]]\n",
      "acc= 0.8981402002861231\n",
      "==================================================\n",
      "Iteration: 9\n",
      "loss= [[0.41788154]]\n",
      "acc= 0.9000223533619456\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(loss_acc_list)):\n",
    "    print('Iteration:',i)\n",
    "    print('loss=',loss_acc_list[i][0])\n",
    "    print('acc=',loss_acc_list[i][1])\n",
    "#     print('time=',loss_acc_list[i][2])\n",
    "    print('==================================================')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get latent factor and Each weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "U, Y, A, E, Au, Ay, Aa, Av, Ave, B =sess.run([user_latent, item_latent, aux_item, embedding, Wu, Wy, Wa, Wv, Wve, Beta])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User latent shape:  (1582, 128)\n",
      "photo latent shape:  (165, 128)\n",
      "Auxilary latent shape:  (165, 128)\n",
      "Wu weight shape: (1582, 165, 128)\n",
      "Wy weight shape: (1582, 165, 128)\n",
      "Wa weight shape: (1582, 165, 128)\n",
      "Wv weight shape: (1582, 165, 200)\n",
      "Wve weight shape: (200, 4876)\n",
      "Embedding shape: (200, 4876)\n",
      "Beta shape: (1582, 200)\n"
     ]
    }
   ],
   "source": [
    "print('User latent shape: ',U.shape)\n",
    "print('photo latent shape: ', Y.shape)\n",
    "print('Auxilary latent shape: ',A.shape)\n",
    "print('Wu weight shape:',Au.shape)\n",
    "print('Wy weight shape:', Ay.shape)\n",
    "print('Wa weight shape:', Aa.shape)\n",
    "print('Wv weight shape:', Av.shape)\n",
    "print('Wve weight shape:', Ave.shape)\n",
    "print('Embedding shape:', E.shape)\n",
    "print('Beta shape:',B.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.savez('./weight/' + SAVE_NAME + '.npz', \n",
    "         U=U, Y=Y, A=A, E=E, Wu=Au, Wy=Ay, Wa=Aa, Wv=Av,B=B,Wve=Ave)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 13\n",
      "alpha:         [74.70322215  0.          0.          0.          0.          0.\n",
      " 33.08789019]\n",
      "softmax alpha: [1.00000000e+00 3.60414984e-33 3.60414984e-33 3.60414984e-33\n",
      " 3.60414984e-33 3.60414984e-33 8.44677580e-19]\n",
      "==================================================\n",
      "1 51\n",
      "alpha:         [ 84.10766201 251.64912589 124.96599081   0.          59.46594414\n",
      "   0.        ]\n",
      "softmax alpha: [1.72848950e-073 1.00000000e+000 9.59872355e-056 5.13066016e-110\n",
      " 3.43482528e-084 5.13066016e-110]\n",
      "==================================================\n",
      "2 54\n",
      "alpha:         [0. 0. 0. 0. 0. 0.]\n",
      "softmax alpha: [0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]\n",
      "==================================================\n",
      "3 61\n",
      "alpha:         [0. 0. 0. 0. 0.]\n",
      "softmax alpha: [0.2 0.2 0.2 0.2 0.2]\n",
      "==================================================\n",
      "4 65\n",
      "alpha:         [  0.         145.24246666  77.89186303 174.69974724   0.\n",
      "  98.80790755]\n",
      "softmax alpha: [1.34543829e-76 1.61014724e-13 9.05461688e-43 1.00000000e+00\n",
      " 1.34543829e-76 1.09797588e-33]\n",
      "==================================================\n",
      "5 88\n",
      "alpha:         [ 55.74230786  94.98787521  76.75199755 237.61000241 124.59009248\n",
      "  21.27745926   0.          76.54024921 137.86060404]\n",
      "softmax alpha: [1.03720314e-079 1.14814611e-062 1.38119858e-070 1.00000000e+000\n",
      " 8.24283819e-050 1.11678676e-094 6.41633617e-104 1.11762211e-070\n",
      " 4.77954678e-044]\n",
      "==================================================\n",
      "6 93\n",
      "alpha:         [0. 0. 0. 0. 0. 0. 0.]\n",
      "softmax alpha: [0.14285714 0.14285714 0.14285714 0.14285714 0.14285714 0.14285714\n",
      " 0.14285714]\n",
      "==================================================\n",
      "7 96\n",
      "alpha:         [  0.          97.12588698 149.79225991   0.           0.        ]\n",
      "softmax alpha: [8.83177692e-66 1.34055568e-23 1.00000000e+00 8.83177692e-66\n",
      " 8.83177692e-66]\n",
      "==================================================\n",
      "8 114\n",
      "alpha:         [ 0.         70.89596651  0.          0.         12.55363572]\n",
      "softmax alpha: [1.62282974e-31 1.00000000e+00 1.62282974e-31 1.62282974e-31\n",
      " 4.59459967e-26]\n",
      "==================================================\n",
      "9 130\n",
      "alpha:         [ 78.5196391  106.08311305  32.32592682   0.          88.76020913]\n",
      "softmax alpha: [1.06988069e-12 9.99999970e-01 9.28241552e-33 8.48573306e-47\n",
      " 2.99749456e-08]\n",
      "==================================================\n",
      "10 135\n",
      "alpha:         [  0.           0.          41.27973486  35.25062141 126.4408847 ]\n",
      "softmax alpha: [1.22298601e-55 1.22298601e-55 1.03510058e-37 2.49213666e-40\n",
      " 1.00000000e+00]\n",
      "==================================================\n",
      "11 142\n",
      "alpha:         [ 0.          0.          0.          0.          0.         23.17061402]\n",
      "softmax alpha: [8.65227249e-11 8.65227249e-11 8.65227249e-11 8.65227249e-11\n",
      " 8.65227249e-11 1.00000000e+00]\n",
      "==================================================\n",
      "12 146\n",
      "alpha:         [36.85740728  0.         90.59085461 10.59229141  0.        ]\n",
      "softmax alpha: [4.61169238e-24 4.53828463e-40 1.00000000e+00 1.80744646e-35\n",
      " 4.53828463e-40]\n",
      "==================================================\n",
      "13 161\n",
      "alpha:         [0. 0. 0. 0. 0.]\n",
      "softmax alpha: [0.2 0.2 0.2 0.2 0.2]\n",
      "==================================================\n",
      "14 163\n",
      "alpha:         [  0.         122.08025448  75.0272955  105.39261786 109.41948638\n",
      " 275.97331874]\n",
      "softmax alpha: [1.40058838e-120 1.46248489e-067 5.37342198e-088 8.27452147e-075\n",
      " 4.64076594e-073 1.00000000e+000]\n",
      "==================================================\n",
      "15 178\n",
      "alpha:         [28.08968627  0.          0.          0.          0.        ]\n",
      "softmax alpha: [1.00000000e+00 6.32126873e-13 6.32126873e-13 6.32126873e-13\n",
      " 6.32126873e-13]\n",
      "==================================================\n",
      "16 186\n",
      "alpha:         [8.3989409 0.        0.        0.        0.        0.       ]\n",
      "softmax alpha: [9.98875737e-01 2.24852530e-04 2.24852530e-04 2.24852530e-04\n",
      " 2.24852530e-04 2.24852530e-04]\n",
      "==================================================\n",
      "17 189\n",
      "alpha:         [  0.           0.         104.24004996  55.77422132  86.06259435]\n",
      "softmax alpha: [5.35946582e-46 5.35946582e-46 9.99999987e-01 8.94454092e-22\n",
      " 1.27535570e-08]\n",
      "==================================================\n",
      "18 191\n",
      "alpha:         [ 71.21218787 158.41551215 229.49058233   0.           0.        ]\n",
      "softmax alpha: [1.82207682e-069 1.35671686e-031 1.00000000e+000 2.15529363e-100\n",
      " 2.15529363e-100]\n",
      "==================================================\n",
      "19 198\n",
      "alpha:         [378.06077045 165.55422769  28.57977656 113.30372855 108.48578875]\n",
      "softmax alpha: [1.00000000e+000 5.12366957e-093 1.66852542e-152 1.04106455e-115\n",
      " 8.41536562e-118]\n",
      "==================================================\n",
      "20 206\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 206 is out of bounds for axis 0 with size 200",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-134e23052f95>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m                                 \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAy\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m                                 \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAa\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_idx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m                                 np.dot(Ave[test_idx[s]],np.expand_dims(all_npy[sample[a]],0).T))))*r\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 206 is out of bounds for axis 0 with size 200"
     ]
    }
   ],
   "source": [
    "'''\n",
    "usr_test_amount = 150\n",
    "movie_test_amount = 32\n",
    "'''\n",
    "\n",
    "#with Embedding\n",
    "result = np.zeros((usr_test_amount, movie_nb))\n",
    "RS = np.zeros((usr_test_amount, movie_nb))\n",
    "#test_idx --> Test 的 index\n",
    "\n",
    "test_yes_id = []\n",
    "\n",
    "for s in range(usr_test_amount):\n",
    "    print(s, test_idx[s])\n",
    "\n",
    "    yes = []\n",
    "    sample = random.sample(train_t[test_idx[s]],len(train_t[test_idx[s]])) #從training part 的positive feedback 取出YouTuber 當成Auxilary\n",
    "    #sample=result_yes_id[now]\n",
    "    test_yes_id.append(sample)\n",
    "    alpha = np.zeros([len(sample)])\n",
    "    \n",
    "    for a in range(len(sample)):\n",
    "        r = np.max(movie_genre[sample[a]] * usr_genre_norm[test_idx[s]]) #sample a 的category vec *user_category vec\n",
    "        #print(test_idx[s])\n",
    "        #print(np.dot(Au[test_idx[s]],np.expand_dims(U[test_idx[s]],0)))\n",
    "\n",
    "        alpha[a] = np.sum((relu(np.dot(Au[test_idx[s]],np.expand_dims(U[test_idx[s]],0).T) +\n",
    "                                np.dot(Ay[sample[a]],np.expand_dims(Y[sample[a]],0).T) +\n",
    "                                np.dot(Aa[test_idx[s]],np.expand_dims(A[sample[a]],0).T) +\n",
    "                                np.dot(Ave[test_idx[s]],np.expand_dims(all_npy[sample[a]],0).T))))*r\n",
    "    \n",
    "        \n",
    "    mul = np.zeros((1,latent_dim))\n",
    "    \n",
    "    print(\"{:<15}{}\".format('alpha:', alpha))\n",
    "    print(\"{:<15}{}\".format('softmax alpha:', softmax(alpha)))\n",
    "    print('==================================================')\n",
    "    \n",
    "    for i in range(len(sample)):\n",
    "        mul += softmax(alpha)[i] * A[sample[i]] #attention alpha*Ai part \n",
    "    new_mul = mul + U[test_idx[s]]  #(U+auxilary)\n",
    "    \n",
    "    for k in range(movie_nb):\n",
    "        result[s][k] = np.dot(new_mul,Y[k].T) #(U+auxilary)*photo latent factor\n",
    "        RS[s][k] = np.dot(new_mul,Y[k].T) + np.dot(B[test_idx[s]], np.dot(E, all_npy[k].T))\n",
    "        \n",
    "#print(RS[s])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#取出test的資料\n",
    "testRS = np.zeros((usr_test_amount, movie_test_amount)) #shape 150*20\n",
    "target = np.zeros((usr_test_amount, movie_test_amount))\n",
    "#test_t 是true的\n",
    "#test_f 是false的\n",
    "        \n",
    "for z in range(usr_test_amount):\n",
    "    user_id = test_idx[z]\n",
    "    #positive target YouTuber list\n",
    "    youtube_t = test_t[z] \n",
    "    #not target YouTuber list\n",
    "    youtube_f = test_f[z]\n",
    "    \n",
    "    #前兩個放target的RS\n",
    "    for i in range(len(youtube_t)):\n",
    "        testRS[z][i] = RS[z][youtube_t[i]]\n",
    "        target[z][i] = 1\n",
    "        \n",
    "    for i in range(len(youtube_f)):\n",
    "        testRS[z][i+len(youtube_t)] = RS[z][youtube_f[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 32) (150, 32)\n"
     ]
    }
   ],
   "source": [
    "print(target.shape, testRS.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of positive data in testing: 1078.0\n",
      "total testing data: 4800\n"
     ]
    }
   ],
   "source": [
    "sumtarget = 0\n",
    "for i in range(len(target)):\n",
    "    #print(np.sum(target[i]))\n",
    "    sumtarget += np.sum(target[i])\n",
    "print('num of positive data in testing:',sumtarget)\n",
    "print('total testing data:', usr_test_amount * movie_test_amount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def topN(sortlist,n):\n",
    "    topList = []\n",
    "    for i in range(n):\n",
    "        topList.append(sortlist.index(max(sortlist)))\n",
    "        #print(max(sortlist))\n",
    "        #print(sortlist.index(max(sortlist)))\n",
    "        sortlist[sortlist.index(max(sortlist))] = -1000000000\n",
    "    return topList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "count_0_all = []\n",
    "for i in range(len(testRS)):\n",
    "    top_0 = topN(list(testRS[i]),int(np.sum(target[i])))\n",
    "    count_0_all.append(top_0)\n",
    "    #print(top_0)\n",
    "\n",
    "acc_0 = 0\n",
    "total = 0\n",
    "for i in range(len(count_0_all)):\n",
    "    for j in range(len(count_0_all[i])):\n",
    "        #print(int(np.sum(target[i])))\n",
    "        total+=int(np.sum(target[i]))\n",
    "        if count_0_all[i][j] < int(np.sum(target[i])): #代表是0或1 (也就是target)\n",
    "            acc_0 += 1\n",
    "avg_acc = acc_0/100\n",
    "\n",
    "print('avg_accuarcy for count_0:', avg_acc)\n",
    "print(acc_0, total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Top n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def F1_score(prec,rec):\n",
    "    f1 = (2*prec*rec)/(prec+rec)\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "correct = 0\n",
    "for i in range(len(testRS)):\n",
    "    top_0 = topN(list(testRS[i]),1) #取一個\n",
    "    count_0_all.append(top_0)\n",
    "    #print(np.sum(target[i]))\n",
    "    #print(top_0)\n",
    "    if top_0[0] < int(np.sum(target[i])):\n",
    "        correct += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prec  0.9333333333333333 recall  0.12987012987012986\n",
      "F1_score: 0.22801302931596087\n"
     ]
    }
   ],
   "source": [
    "top1_prec = correct/len(testRS)\n",
    "top1_recall = correct/(sumtarget)\n",
    "print('prec ',top1_prec,'recall ',top1_recall)\n",
    "print('F1_score:',F1_score(top1_prec,top1_recall))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "correct = 0\n",
    "for i in range(len(testRS)):\n",
    "    top_3 = topN(list(testRS[i]),3) #取一個\n",
    "    count_0_all.append(top_3)\n",
    "    #print(top_3)\n",
    "    for j in range(len(top_3)):\n",
    "        if top_3[j] < int(np.sum(target[i])):\n",
    "            correct += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prec  0.9155555555555556 recall  0.3821892393320965\n",
      "F1_score: 0.5392670157068064\n"
     ]
    }
   ],
   "source": [
    "top3_prec = correct/(len(testRS)*3)\n",
    "top3_recall = correct/(sumtarget)\n",
    "print('prec ',top3_prec,'recall ',top3_recall)\n",
    "print('F1_score:',F1_score(top3_prec,top3_recall))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "correct = 0\n",
    "for i in range(len(testRS)):\n",
    "    top_5 = topN(list(testRS[i]),5) #取一個\n",
    "    count_0_all.append(top_5)\n",
    "    #print(top_5)\n",
    "    for j in range(len(top_5)):\n",
    "        if top_5[j] < int(np.sum(target[i])):\n",
    "            correct += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prec  0.9173333333333333 recall  0.6382189239332097\n",
      "F1_score: 0.7527352297592997\n"
     ]
    }
   ],
   "source": [
    "top5_prec = correct/(len(testRS)*5)\n",
    "top5_recall = correct/(sumtarget)\n",
    "print('prec ',top5_prec,'recall ',top5_recall)\n",
    "print('F1_score:',F1_score(top5_prec,top5_recall))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pre_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "usr_test_amount = 150\n",
    "movie_test_amount = 16\n",
    "'''\n",
    "all_sort = []\n",
    "pre_matrix = np.zeros(shape=(usr_test_amount, movie_test_amount))\n",
    "for i in range(usr_test_amount):\n",
    "    top_5 = topN(list(testRS[i]),5) #取一個\n",
    "    #print(top_5)\n",
    "    all_sort.append(topN(list(testRS[i]),len(testRS[i])))\n",
    "    for j in range(len(top_5)):\n",
    "        pre_matrix[i][top_5[j]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 32) (150, 32)\n"
     ]
    }
   ],
   "source": [
    "print(pre_matrix.shape, target.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NDCG\n",
    "* https://daiwk.github.io/posts/nlp-ndcg.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Ideal DCG，理想状况下的DCG。也就是说，相关性完全由高到低排序时算出的DCG：\n",
    "\n",
    "def IDCG(ideal_list): #ideal_list example = [1,1,1,1,1,0,0,....]\n",
    "    idcg = 0\n",
    "    for i in range(len(ideal_list)):\n",
    "        #print((2**true_list[i]-1),math.log2(i+2))\n",
    "        idcg += (2**ideal_list[i]-1)/math.log2(i+2)\n",
    "    #print('idcg',idcg)\n",
    "    return idcg\n",
    "\n",
    "def DCG(prec_list): #找出前n名的[1,1,1,0,...]\n",
    "    dcg = 0\n",
    "    for i in range(len(prec_list)):\n",
    "        dcg += (2**prec_list[i]-1)/math.log2(i+2)\n",
    "    #print('dcg',dcg)\n",
    "    return dcg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NDCG: 0.9199021358837534\n"
     ]
    }
   ],
   "source": [
    "total_ndcg = 0\n",
    "num_ndcg = 5\n",
    "for m in range(usr_test_amount):\n",
    "    idcg = IDCG([1]*num_ndcg)\n",
    "    pre_list = []\n",
    "    for s in all_sort[m][:num_ndcg]:\n",
    "        #print(s)\n",
    "        #print(target[m][s])\n",
    "        pre_list.append(target[m][s])\n",
    "    dcg = DCG(pre_list)\n",
    "    ndcg = dcg/idcg\n",
    "    #print(ndcg)\n",
    "    total_ndcg += ndcg\n",
    "avg_ndcg = total_ndcg/usr_test_amount\n",
    "print('NDCG:',avg_ndcg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import average_precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP: 0.7533307686866504\n"
     ]
    }
   ],
   "source": [
    "total_prec = 0\n",
    "for u in range(usr_test_amount):\n",
    "    y_true = target[u]\n",
    "    y_scores = pre_matrix[u]\n",
    "    total_prec += average_precision_score(y_true, y_scores)\n",
    "    \n",
    "MAP = total_prec/usr_test_amount\n",
    "\n",
    "print('MAP:', MAP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
